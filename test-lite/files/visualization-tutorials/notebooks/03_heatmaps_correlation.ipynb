{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de6d70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic Dataset Download for Heatmap and Correlation Analysis\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "def create_heatmap_datasets():\n",
    "    \"\"\"Create comprehensive datasets for heatmap and correlation visualization\"\"\"\n",
    "    np.random.seed(456)  # For reproducible heatmaps\n",
    "    \n",
    "    data_dir = Path(\"../data\")\n",
    "    heatmap_dir = data_dir / \"heatmap_data\"\n",
    "    heatmap_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    datasets = {}\n",
    "    \n",
    "    # 1. Gene expression-like matrix for pathology markers\n",
    "    print(\"üß¨ Creating pathology marker expression matrix...\")\n",
    "    \n",
    "    n_samples = 200\n",
    "    n_markers = 25\n",
    "    \n",
    "    # Define marker categories with realistic names\n",
    "    immune_markers = ['CD3', 'CD8', 'CD4', 'FOXP3', 'CD68', 'CD163', 'PD1', 'PDL1']\n",
    "    proliferation_markers = ['Ki67', 'PCNA', 'MCM2', 'Cyclin_D1', 'p53']\n",
    "    apoptosis_markers = ['Caspase3', 'BAX', 'BCL2', 'p21', 'TUNEL']\n",
    "    structural_markers = ['CK7', 'CK20', 'Vimentin', 'SMA', 'Collagen_I', 'Fibronectin', 'E_cadherin']\n",
    "    \n",
    "    all_markers = immune_markers + proliferation_markers + apoptosis_markers + structural_markers\n",
    "    \n",
    "    # Create correlated marker expression\n",
    "    marker_expression = np.zeros((n_samples, len(all_markers)))\n",
    "    \n",
    "    for i, marker in enumerate(all_markers):\n",
    "        if marker in immune_markers:\n",
    "            base_expression = np.random.lognormal(2, 1, n_samples)\n",
    "        elif marker in proliferation_markers:\n",
    "            base_expression = np.random.gamma(2, 2, n_samples)\n",
    "        elif marker in apoptosis_markers:\n",
    "            base_expression = np.random.exponential(1.5, n_samples)\n",
    "        else:  # structural markers\n",
    "            base_expression = np.random.beta(2, 3, n_samples) * 10\n",
    "        \n",
    "        marker_expression[:, i] = base_expression\n",
    "    \n",
    "    # Add some realistic correlations\n",
    "    # Immune markers correlate with each other\n",
    "    for i in range(len(immune_markers)):\n",
    "        for j in range(i+1, len(immune_markers)):\n",
    "            correlation_strength = np.random.uniform(0.3, 0.7)\n",
    "            noise = np.random.normal(0, 0.1, n_samples)\n",
    "            marker_expression[:, j] = (correlation_strength * marker_expression[:, i] + \n",
    "                                     (1-correlation_strength) * marker_expression[:, j] + noise)\n",
    "    \n",
    "    expression_df = pd.DataFrame(marker_expression, columns=all_markers)\n",
    "    expression_df['sample_id'] = [f'S{i:03d}' for i in range(n_samples)]\n",
    "    expression_df['tissue_type'] = np.random.choice(['Normal', 'Tumor', 'Metastasis'], \n",
    "                                                   n_samples, p=[0.3, 0.5, 0.2])\n",
    "    expression_df['grade'] = np.random.choice([1, 2, 3], n_samples, p=[0.3, 0.5, 0.2])\n",
    "    \n",
    "    expression_file = heatmap_dir / \"marker_expression_matrix.csv\"\n",
    "    expression_df.to_csv(expression_file, index=False)\n",
    "    datasets['marker_expression'] = str(expression_file)\n",
    "    \n",
    "    # 2. Correlation matrix between clinical variables\n",
    "    print(\"üè• Building clinical correlation dataset...\")\n",
    "    \n",
    "    n_patients = 300\n",
    "    clinical_vars = ['age', 'tumor_size', 'lymph_nodes', 'grade', 'stage_numeric', \n",
    "                    'ki67', 'er_score', 'pr_score', 'her2_score', 'survival_months']\n",
    "    \n",
    "    # Define correlation structure\n",
    "    correlation_matrix = np.array([\n",
    "        [1.00, 0.15, 0.20, 0.25, 0.30, 0.05, -0.10, -0.05, 0.00, -0.45],  # age\n",
    "        [0.15, 1.00, 0.60, 0.70, 0.75, 0.50, -0.20, -0.15, 0.25, -0.65],  # tumor_size\n",
    "        [0.20, 0.60, 1.00, 0.55, 0.80, 0.35, -0.30, -0.25, 0.15, -0.70],  # lymph_nodes\n",
    "        [0.25, 0.70, 0.55, 1.00, 0.65, 0.60, -0.40, -0.35, 0.30, -0.55],  # grade\n",
    "        [0.30, 0.75, 0.80, 0.65, 1.00, 0.45, -0.35, -0.30, 0.20, -0.75],  # stage\n",
    "        [0.05, 0.50, 0.35, 0.60, 0.45, 1.00, -0.50, -0.45, 0.10, -0.40],  # ki67\n",
    "        [-0.10, -0.20, -0.30, -0.40, -0.35, -0.50, 1.00, 0.70, -0.15, 0.35],  # er_score\n",
    "        [-0.05, -0.15, -0.25, -0.35, -0.30, -0.45, 0.70, 1.00, -0.10, 0.30],  # pr_score\n",
    "        [0.00, 0.25, 0.15, 0.30, 0.20, 0.10, -0.15, -0.10, 1.00, -0.25],  # her2_score\n",
    "        [-0.45, -0.65, -0.70, -0.55, -0.75, -0.40, 0.35, 0.30, -0.25, 1.00]   # survival\n",
    "    ])\n",
    "    \n",
    "    # Generate correlated clinical data\n",
    "    mvn_data = np.random.multivariate_normal(\n",
    "        mean=np.zeros(len(clinical_vars)),\n",
    "        cov=correlation_matrix,\n",
    "        size=n_patients\n",
    "    )\n",
    "    \n",
    "    # Transform to realistic ranges\n",
    "    clinical_data = {\n",
    "        'patient_id': [f'P{i:03d}' for i in range(n_patients)],\n",
    "        'age': np.clip(65 + mvn_data[:, 0] * 12, 25, 90).astype(int),\n",
    "        'tumor_size': np.maximum(5, 25 + mvn_data[:, 1] * 15),\n",
    "        'lymph_nodes': np.maximum(0, 2 + mvn_data[:, 2] * 4).astype(int),\n",
    "        'grade': np.clip(2 + mvn_data[:, 3] * 0.8, 1, 3).astype(int),\n",
    "        'stage_numeric': np.clip(2 + mvn_data[:, 4] * 1.2, 1, 4).astype(int),\n",
    "        'ki67': np.clip(20 + mvn_data[:, 5] * 25, 0, 100),\n",
    "        'er_score': np.clip(50 + mvn_data[:, 6] * 40, 0, 100),\n",
    "        'pr_score': np.clip(40 + mvn_data[:, 7] * 35, 0, 100),\n",
    "        'her2_score': np.clip(1.5 + mvn_data[:, 8] * 1, 0, 3),\n",
    "        'survival_months': np.maximum(3, 36 + mvn_data[:, 9] * 24)\n",
    "    }\n",
    "    \n",
    "    clinical_df = pd.DataFrame(clinical_data)\n",
    "    clinical_file = heatmap_dir / \"clinical_correlations.csv\"\n",
    "    clinical_df.to_csv(clinical_file, index=False)\n",
    "    datasets['clinical_correlations'] = str(clinical_file)\n",
    "    \n",
    "    # 3. Time-series heatmap data (treatments over time)\n",
    "    print(\"‚è∞ Creating treatment response time-series...\")\n",
    "    \n",
    "    treatments = ['Surgery', 'Chemotherapy', 'Radiation', 'Immunotherapy', 'Hormone_therapy']\n",
    "    time_points = ['Baseline', '1_month', '3_months', '6_months', '12_months', '24_months']\n",
    "    \n",
    "    timeseries_data = []\n",
    "    for patient in range(100):\n",
    "        for treatment in treatments:\n",
    "            for time_point in time_points:\n",
    "                # Simulate treatment response over time\n",
    "                time_index = time_points.index(time_point)\n",
    "                base_response = np.random.normal(50, 15)\n",
    "                \n",
    "                # Different treatments have different response patterns\n",
    "                if treatment == 'Surgery':\n",
    "                    response = base_response + (time_index * 10) + np.random.normal(0, 5)\n",
    "                elif treatment == 'Chemotherapy':\n",
    "                    response = base_response + (20 - time_index * 2) + np.random.normal(0, 8)\n",
    "                elif treatment == 'Radiation':\n",
    "                    response = base_response + (15 - time_index * 1.5) + np.random.normal(0, 6)\n",
    "                elif treatment == 'Immunotherapy':\n",
    "                    response = base_response + (time_index * 5) + np.random.normal(0, 10)\n",
    "                else:  # Hormone therapy\n",
    "                    response = base_response + (time_index * 3) + np.random.normal(0, 7)\n",
    "                \n",
    "                timeseries_data.append({\n",
    "                    'patient_id': f'T{patient:03d}',\n",
    "                    'treatment': treatment,\n",
    "                    'time_point': time_point,\n",
    "                    'response_score': max(0, min(100, response)),\n",
    "                    'toxicity_score': np.random.beta(1, 3) * 50\n",
    "                })\n",
    "    \n",
    "    timeseries_df = pd.DataFrame(timeseries_data)\n",
    "    timeseries_file = heatmap_dir / \"treatment_timeseries.csv\"\n",
    "    timeseries_df.to_csv(timeseries_file, index=False)\n",
    "    datasets['treatment_timeseries'] = str(timeseries_file)\n",
    "    \n",
    "    # Save dataset metadata\n",
    "    metadata = {\n",
    "        'creation_info': {\n",
    "            'date': pd.Timestamp.now().isoformat(),\n",
    "            'purpose': 'Heatmap and correlation visualization examples',\n",
    "            'seed': 456\n",
    "        },\n",
    "        'datasets': {\n",
    "            'marker_expression': {\n",
    "                'description': 'Pathology marker expression matrix',\n",
    "                'dimensions': f\"{n_samples} samples √ó {len(all_markers)} markers\",\n",
    "                'marker_categories': {\n",
    "                    'immune': immune_markers,\n",
    "                    'proliferation': proliferation_markers,\n",
    "                    'apoptosis': apoptosis_markers,\n",
    "                    'structural': structural_markers\n",
    "                }\n",
    "            },\n",
    "            'clinical_correlations': {\n",
    "                'description': 'Correlated clinical variables',\n",
    "                'n_patients': n_patients,\n",
    "                'variables': clinical_vars,\n",
    "                'correlation_structure': 'Realistic clinical relationships'\n",
    "            },\n",
    "            'treatment_timeseries': {\n",
    "                'description': 'Treatment response over time',\n",
    "                'treatments': treatments,\n",
    "                'time_points': time_points,\n",
    "                'n_patients': 100\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    metadata_file = heatmap_dir / \"heatmap_metadata.json\"\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"üéØ Heatmap datasets ready! {len(datasets)} datasets created\")\n",
    "    print(f\"üìÅ Data location: {heatmap_dir.absolute()}\")\n",
    "    \n",
    "    return heatmap_dir, datasets, metadata\n",
    "\n",
    "# Generate heatmap and correlation datasets\n",
    "heatmap_data_dir, available_heatmap_datasets, heatmap_metadata = create_heatmap_datasets()\n",
    "\n",
    "print(\"üó∫Ô∏è Available heatmap datasets:\")\n",
    "for name, path in available_heatmap_datasets.items():\n",
    "    print(f\"  ‚Ä¢ {name}: {Path(path).name}\")\n",
    "\n",
    "print(\"\\nüî• Ready for heatmap and correlation visualization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109115c5",
   "metadata": {},
   "source": [
    "# Heatmaps and Correlation Analysis for Digital Pathology\n",
    "\n",
    "This notebook focuses on advanced correlation analysis and heatmap visualizations specifically designed for digital pathology research. You'll learn to create publication-quality heatmaps and perform comprehensive correlation studies.\n",
    "\n",
    "## Learning Objectives\n",
    "1. Create correlation matrices for pathology features\n",
    "2. Generate advanced heatmaps with clustering\n",
    "3. Perform partial correlation analysis\n",
    "4. Visualize feature importance and biomarker relationships\n",
    "5. Create interactive correlation plots\n",
    "6. Handle missing data in correlation analysis\n",
    "\n",
    "## Prerequisites\n",
    "- Completed previous visualization notebooks\n",
    "- Understanding of correlation concepts\n",
    "- Basic knowledge of pathology feature types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad95f1d7",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa06ab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0b2b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive pathology dataset for correlation analysis\n",
    "np.random.seed(42)\n",
    "n_patients = 300\n",
    "\n",
    "# Generate correlated pathology features\n",
    "# Nuclear morphology features (these should be correlated)\n",
    "nuclear_area = np.random.normal(150, 30, n_patients)\n",
    "nuclear_perimeter = 2 * np.sqrt(np.pi * nuclear_area) + np.random.normal(0, 5, n_patients)\n",
    "nuclear_compactness = (4 * np.pi * nuclear_area) / (nuclear_perimeter ** 2) + np.random.normal(0, 0.1, n_patients)\n",
    "nuclear_eccentricity = np.random.uniform(0.1, 0.9, n_patients)\n",
    "\n",
    "# Chromatin texture features (partially correlated)\n",
    "chromatin_density = np.random.normal(0.6, 0.15, n_patients)\n",
    "chromatin_homogeneity = 1 - chromatin_density * 0.3 + np.random.normal(0, 0.1, n_patients)\n",
    "chromatin_contrast = chromatin_density * 1.5 + np.random.normal(0, 0.2, n_patients)\n",
    "\n",
    "# Cell proliferation markers (should correlate with malignancy)\n",
    "mitotic_count = np.random.poisson(3, n_patients)\n",
    "ki67_index = mitotic_count * 2 + np.random.normal(15, 5, n_patients)\n",
    "ki67_index = np.clip(ki67_index, 0, 100)\n",
    "\n",
    "# Vascular features\n",
    "vessel_density = np.random.normal(20, 5, n_patients)\n",
    "vessel_area = vessel_density * 0.8 + np.random.normal(0, 2, n_patients)\n",
    "\n",
    "# Inflammatory markers\n",
    "lymphocyte_count = np.random.poisson(50, n_patients)\n",
    "macrophage_count = lymphocyte_count * 0.3 + np.random.poisson(15, n_patients)\n",
    "\n",
    "# Staining intensities (H&E and IHC)\n",
    "hematoxylin_intensity = np.random.normal(0.6, 0.1, n_patients)\n",
    "eosin_intensity = np.random.normal(0.4, 0.1, n_patients)\n",
    "dab_intensity = np.random.normal(0.5, 0.15, n_patients)  # DAB for IHC\n",
    "\n",
    "# Clinical/demographic variables\n",
    "age = np.random.normal(65, 12, n_patients).astype(int)\n",
    "tumor_grade = np.random.choice([1, 2, 3], n_patients, p=[0.2, 0.5, 0.3])\n",
    "tumor_stage = np.random.choice([1, 2, 3, 4], n_patients, p=[0.3, 0.4, 0.2, 0.1])\n",
    "\n",
    "# Survival data (months)\n",
    "survival_time = np.random.exponential(36, n_patients)  # months\n",
    "survival_time = np.where(tumor_grade == 3, survival_time * 0.6, survival_time)  # Grade 3 = worse survival\n",
    "\n",
    "# Create DataFrame\n",
    "pathology_data = pd.DataFrame({\n",
    "    'patient_id': [f'P{i:04d}' for i in range(1, n_patients + 1)],\n",
    "    'age': age,\n",
    "    'tumor_grade': tumor_grade,\n",
    "    'tumor_stage': tumor_stage,\n",
    "    'nuclear_area': nuclear_area,\n",
    "    'nuclear_perimeter': nuclear_perimeter,\n",
    "    'nuclear_compactness': nuclear_compactness,\n",
    "    'nuclear_eccentricity': nuclear_eccentricity,\n",
    "    'chromatin_density': chromatin_density,\n",
    "    'chromatin_homogeneity': chromatin_homogeneity,\n",
    "    'chromatin_contrast': chromatin_contrast,\n",
    "    'mitotic_count': mitotic_count,\n",
    "    'ki67_index': ki67_index,\n",
    "    'vessel_density': vessel_density,\n",
    "    'vessel_area': vessel_area,\n",
    "    'lymphocyte_count': lymphocyte_count,\n",
    "    'macrophage_count': macrophage_count,\n",
    "    'hematoxylin_intensity': hematoxylin_intensity,\n",
    "    'eosin_intensity': eosin_intensity,\n",
    "    'dab_intensity': dab_intensity,\n",
    "    'survival_time': survival_time\n",
    "})\n",
    "\n",
    "# Add some missing values to make it realistic\n",
    "missing_indices = np.random.choice(pathology_data.index, size=int(0.05 * len(pathology_data)), replace=False)\n",
    "pathology_data.loc[missing_indices, 'ki67_index'] = np.nan\n",
    "\n",
    "missing_indices_2 = np.random.choice(pathology_data.index, size=int(0.03 * len(pathology_data)), replace=False)\n",
    "pathology_data.loc[missing_indices_2, 'dab_intensity'] = np.nan\n",
    "\n",
    "print(f\"‚úÖ Pathology dataset created with {len(pathology_data)} patients\")\n",
    "print(f\"Features: {pathology_data.shape[1]} columns\")\n",
    "print(f\"Missing values: {pathology_data.isnull().sum().sum()} total\")\n",
    "pathology_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48dbc03",
   "metadata": {},
   "source": [
    "## 2. Basic Correlation Matrix and Heatmap\n",
    "\n",
    "Start with fundamental correlation analysis and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80562e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical features for correlation analysis\n",
    "numerical_features = pathology_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numerical_features.remove('patient_id') if 'patient_id' in numerical_features else None\n",
    "\n",
    "print(f\"Analyzing correlations for {len(numerical_features)} numerical features:\")\n",
    "for i, feature in enumerate(numerical_features, 1):\n",
    "    print(f\"{i:2d}. {feature}\")\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = pathology_data[numerical_features].corr()\n",
    "\n",
    "print(f\"\\n‚úÖ Correlation matrix calculated ({correlation_matrix.shape})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae7d80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create basic correlation heatmap\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Basic heatmap\n",
    "sns.heatmap(correlation_matrix, \n",
    "            annot=True, \n",
    "            cmap='RdBu_r', \n",
    "            center=0, \n",
    "            vmin=-1, vmax=1,\n",
    "            square=True,\n",
    "            linewidths=0.5,\n",
    "            cbar_kws={\"shrink\": 0.8},\n",
    "            annot_kws={'size': 8},\n",
    "            ax=axes[0])\n",
    "axes[0].set_title('Complete Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].tick_params(axis='y', rotation=0)\n",
    "\n",
    "# Focus on strong correlations only (|r| > 0.3)\n",
    "strong_corr_mask = np.abs(correlation_matrix) < 0.3\n",
    "masked_correlation = correlation_matrix.copy()\n",
    "masked_correlation[strong_corr_mask] = 0\n",
    "\n",
    "sns.heatmap(masked_correlation, \n",
    "            annot=True, \n",
    "            cmap='RdBu_r', \n",
    "            center=0, \n",
    "            vmin=-1, vmax=1,\n",
    "            square=True,\n",
    "            linewidths=0.5,\n",
    "            cbar_kws={\"shrink\": 0.8},\n",
    "            annot_kws={'size': 8},\n",
    "            ax=axes[1])\n",
    "axes[1].set_title('Strong Correlations Only (|r| ‚â• 0.3)', fontsize=14, fontweight='bold')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].tick_params(axis='y', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print strongest correlations\n",
    "print(\"=== STRONGEST CORRELATIONS ===\")\n",
    "# Get upper triangle to avoid duplicates\n",
    "upper_triangle = correlation_matrix.where(np.triu(np.ones_like(correlation_matrix, dtype=bool), k=1))\n",
    "strong_correlations = upper_triangle.stack().sort_values(key=abs, ascending=False)\n",
    "\n",
    "print(\"Top 10 strongest correlations:\")\n",
    "for i, (pair, corr_value) in enumerate(strong_correlations.head(10).items(), 1):\n",
    "    feature1, feature2 = pair\n",
    "    strength = \"Very Strong\" if abs(corr_value) > 0.7 else \"Strong\" if abs(corr_value) > 0.5 else \"Moderate\"\n",
    "    direction = \"Positive\" if corr_value > 0 else \"Negative\"\n",
    "    print(f\"{i:2d}. {feature1} ‚Üî {feature2}: {corr_value:.3f} ({strength} {direction})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f01a061",
   "metadata": {},
   "source": [
    "## 3. Advanced Clustered Heatmap\n",
    "\n",
    "Create heatmaps with hierarchical clustering to group related features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208f3617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create clustered heatmap using hierarchical clustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "# Calculate distance matrix (1 - |correlation|)\n",
    "distance_matrix = 1 - np.abs(correlation_matrix)\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "linkage_matrix = linkage(squareform(distance_matrix), method='average')\n",
    "\n",
    "# Create clustered heatmap\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "\n",
    "# 1. Dendrogram\n",
    "dendrogram(linkage_matrix, labels=correlation_matrix.columns, \n",
    "           orientation='left', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Feature Dendrogram', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 2. Clustered correlation matrix\n",
    "from scipy.cluster.hierarchy import leaves_list\n",
    "cluster_order = leaves_list(linkage_matrix)\n",
    "clustered_corr = correlation_matrix.iloc[cluster_order, cluster_order]\n",
    "\n",
    "sns.heatmap(clustered_corr, \n",
    "            annot=True, \n",
    "            cmap='RdBu_r', \n",
    "            center=0,\n",
    "            vmin=-1, vmax=1,\n",
    "            square=True,\n",
    "            linewidths=0.5,\n",
    "            annot_kws={'size': 7},\n",
    "            ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Clustered Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].tick_params(axis='y', rotation=0)\n",
    "\n",
    "# 3. Feature groups identification\n",
    "n_clusters = 4  # Number of feature groups\n",
    "cluster_labels = fcluster(linkage_matrix, n_clusters, criterion='maxclust')\n",
    "\n",
    "# Create feature groups\n",
    "feature_groups = {}\n",
    "for i, feature in enumerate(correlation_matrix.columns):\n",
    "    cluster_id = cluster_labels[i]\n",
    "    if cluster_id not in feature_groups:\n",
    "        feature_groups[cluster_id] = []\n",
    "    feature_groups[cluster_id].append(feature)\n",
    "\n",
    "print(\"=== IDENTIFIED FEATURE GROUPS ===\")\n",
    "group_colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "for group_id, features in feature_groups.items():\n",
    "    print(f\"\\nGroup {group_id} ({len(features)} features):\")\n",
    "    for feature in features:\n",
    "        print(f\"  ‚Ä¢ {feature}\")\n",
    "\n",
    "# 4. Group-wise correlation analysis\n",
    "group_correlations = []\n",
    "for group_id, features in feature_groups.items():\n",
    "    if len(features) > 1:\n",
    "        group_corr_matrix = correlation_matrix.loc[features, features]\n",
    "        # Calculate mean within-group correlation\n",
    "        upper_tri = group_corr_matrix.where(np.triu(np.ones_like(group_corr_matrix, dtype=bool), k=1))\n",
    "        mean_corr = upper_tri.stack().mean()\n",
    "        group_correlations.append({'Group': group_id, 'Mean_Correlation': mean_corr, 'Features': features})\n",
    "\n",
    "# Visualize group correlations\n",
    "if group_correlations:\n",
    "    group_df = pd.DataFrame(group_correlations)\n",
    "    axes[1, 0].bar(range(len(group_df)), group_df['Mean_Correlation'], color=group_colors[:len(group_df)])\n",
    "    axes[1, 0].set_xlabel('Feature Group')\n",
    "    axes[1, 0].set_ylabel('Mean Within-Group Correlation')\n",
    "    axes[1, 0].set_title('Within-Group Correlation Strength')\n",
    "    axes[1, 0].set_xticks(range(len(group_df)))\n",
    "    axes[1, 0].set_xticklabels([f'Group {g}' for g in group_df['Group']])\n",
    "\n",
    "# 5. Cross-group correlation heatmap\n",
    "cross_group_corr = np.zeros((len(feature_groups), len(feature_groups)))\n",
    "group_names = []\n",
    "\n",
    "for i, (group1_id, features1) in enumerate(feature_groups.items()):\n",
    "    group_names.append(f'Group {group1_id}')\n",
    "    for j, (group2_id, features2) in enumerate(feature_groups.items()):\n",
    "        if i != j:  # Different groups\n",
    "            cross_corr_values = []\n",
    "            for f1 in features1:\n",
    "                for f2 in features2:\n",
    "                    cross_corr_values.append(abs(correlation_matrix.loc[f1, f2]))\n",
    "            cross_group_corr[i, j] = np.mean(cross_corr_values) if cross_corr_values else 0\n",
    "\n",
    "sns.heatmap(cross_group_corr, \n",
    "            annot=True, \n",
    "            cmap='YlOrRd',\n",
    "            xticklabels=group_names,\n",
    "            yticklabels=group_names,\n",
    "            square=True,\n",
    "            ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Cross-Group Correlation Strength', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669ee672",
   "metadata": {},
   "source": [
    "## 4. Feature-Specific Correlation Analysis\n",
    "\n",
    "Analyze correlations for different types of pathology features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee9919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature categories based on biological relevance\n",
    "feature_categories = {\n",
    "    'Nuclear Morphology': ['nuclear_area', 'nuclear_perimeter', 'nuclear_compactness', 'nuclear_eccentricity'],\n",
    "    'Chromatin Features': ['chromatin_density', 'chromatin_homogeneity', 'chromatin_contrast'],\n",
    "    'Proliferation Markers': ['mitotic_count', 'ki67_index'],\n",
    "    'Vascular Features': ['vessel_density', 'vessel_area'],\n",
    "    'Immune Markers': ['lymphocyte_count', 'macrophage_count'],\n",
    "    'Staining Intensities': ['hematoxylin_intensity', 'eosin_intensity', 'dab_intensity'],\n",
    "    'Clinical Variables': ['age', 'tumor_grade', 'tumor_stage', 'survival_time']\n",
    "}\n",
    "\n",
    "# Create category-based analysis\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "category_stats = []\n",
    "\n",
    "for i, (category, features) in enumerate(feature_categories.items()):\n",
    "    if i < len(axes):\n",
    "        # Filter features that exist in dataset\n",
    "        available_features = [f for f in features if f in pathology_data.columns]\n",
    "        \n",
    "        if len(available_features) > 1:\n",
    "            # Calculate correlation matrix for this category\n",
    "            cat_corr = pathology_data[available_features].corr()\n",
    "            \n",
    "            # Create heatmap\n",
    "            sns.heatmap(cat_corr, \n",
    "                       annot=True, \n",
    "                       cmap='RdBu_r', \n",
    "                       center=0,\n",
    "                       vmin=-1, vmax=1,\n",
    "                       square=True,\n",
    "                       linewidths=1,\n",
    "                       annot_kws={'size': 10},\n",
    "                       ax=axes[i])\n",
    "            axes[i].set_title(f'{category}\\n({len(available_features)} features)', \n",
    "                             fontsize=12, fontweight='bold')\n",
    "            axes[i].tick_params(axis='x', rotation=45)\n",
    "            axes[i].tick_params(axis='y', rotation=0)\n",
    "            \n",
    "            # Calculate statistics\n",
    "            upper_tri = cat_corr.where(np.triu(np.ones_like(cat_corr, dtype=bool), k=1))\n",
    "            correlations = upper_tri.stack()\n",
    "            \n",
    "            category_stats.append({\n",
    "                'Category': category,\n",
    "                'N_Features': len(available_features),\n",
    "                'Mean_Correlation': correlations.mean(),\n",
    "                'Max_Correlation': correlations.max(),\n",
    "                'Min_Correlation': correlations.min(),\n",
    "                'Strong_Correlations': sum(abs(correlations) > 0.5)\n",
    "            })\n",
    "\n",
    "# Remove empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display category statistics\n",
    "if category_stats:\n",
    "    stats_df = pd.DataFrame(category_stats)\n",
    "    print(\"=== CATEGORY-WISE CORRELATION STATISTICS ===\")\n",
    "    print(stats_df.round(3).to_string(index=False))\n",
    "    \n",
    "    # Visualize category statistics\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Mean correlations by category\n",
    "    axes[0].barh(stats_df['Category'], stats_df['Mean_Correlation'], color='skyblue')\n",
    "    axes[0].set_xlabel('Mean Correlation')\n",
    "    axes[0].set_title('Average Correlation Strength by Feature Category')\n",
    "    axes[0].axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Number of strong correlations\n",
    "    axes[1].barh(stats_df['Category'], stats_df['Strong_Correlations'], color='coral')\n",
    "    axes[1].set_xlabel('Number of Strong Correlations (|r| > 0.5)')\n",
    "    axes[1].set_title('Strong Correlations Count by Category')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72932841",
   "metadata": {},
   "source": [
    "## 5. Partial Correlation Analysis\n",
    "\n",
    "Analyze correlations while controlling for confounding variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432c4422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial correlation analysis\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def partial_correlation(df, x, y, control_vars):\n",
    "    \"\"\"\n",
    "    Calculate partial correlation between x and y, controlling for control_vars\n",
    "    \"\"\"\n",
    "    # Remove rows with missing values\n",
    "    analysis_data = df[[x, y] + control_vars].dropna()\n",
    "    \n",
    "    if len(analysis_data) < 10:  # Need sufficient data\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    # Standardize all variables\n",
    "    scaler = StandardScaler()\n",
    "    standardized_data = scaler.fit_transform(analysis_data)\n",
    "    standardized_df = pd.DataFrame(standardized_data, columns=[x, y] + control_vars)\n",
    "    \n",
    "    # Calculate residuals after regressing out control variables\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "    reg_model = LinearRegression()\n",
    "    \n",
    "    # Residuals for x after controlling for control_vars\n",
    "    if control_vars:\n",
    "        reg_model.fit(standardized_df[control_vars], standardized_df[x])\n",
    "        x_residuals = standardized_df[x] - reg_model.predict(standardized_df[control_vars])\n",
    "        \n",
    "        # Residuals for y after controlling for control_vars\n",
    "        reg_model.fit(standardized_df[control_vars], standardized_df[y])\n",
    "        y_residuals = standardized_df[y] - reg_model.predict(standardized_df[control_vars])\n",
    "    else:\n",
    "        x_residuals = standardized_df[x]\n",
    "        y_residuals = standardized_df[y]\n",
    "    \n",
    "    # Calculate correlation of residuals\n",
    "    return pearsonr(x_residuals, y_residuals)\n",
    "\n",
    "# Perform partial correlation analysis\n",
    "print(\"=== PARTIAL CORRELATION ANALYSIS ===\")\n",
    "print(\"Controlling for age, tumor_grade, and tumor_stage\")\n",
    "\n",
    "# Define pairs to analyze\n",
    "correlation_pairs = [\n",
    "    ('nuclear_area', 'mitotic_count'),\n",
    "    ('chromatin_density', 'ki67_index'),\n",
    "    ('vessel_density', 'lymphocyte_count'),\n",
    "    ('nuclear_compactness', 'survival_time'),\n",
    "    ('chromatin_contrast', 'tumor_grade')\n",
    "]\n",
    "\n",
    "control_variables = ['age', 'tumor_grade', 'tumor_stage']\n",
    "\n",
    "partial_corr_results = []\n",
    "\n",
    "print(f\"\\n{'Pair':<40} {'Raw Corr':<12} {'Partial Corr':<12} {'P-value':<12} {'Change'}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "for x, y in correlation_pairs:\n",
    "    if x in pathology_data.columns and y in pathology_data.columns:\n",
    "        # Raw correlation\n",
    "        raw_data = pathology_data[[x, y]].dropna()\n",
    "        if len(raw_data) > 0:\n",
    "            raw_corr, raw_p = pearsonr(raw_data[x], raw_data[y])\n",
    "        else:\n",
    "            raw_corr, raw_p = np.nan, np.nan\n",
    "        \n",
    "        # Partial correlation\n",
    "        partial_corr, partial_p = partial_correlation(pathology_data, x, y, control_variables)\n",
    "        \n",
    "        # Calculate change\n",
    "        change = partial_corr - raw_corr if not (np.isnan(raw_corr) or np.isnan(partial_corr)) else np.nan\n",
    "        \n",
    "        partial_corr_results.append({\n",
    "            'Variable_1': x,\n",
    "            'Variable_2': y,\n",
    "            'Raw_Correlation': raw_corr,\n",
    "            'Partial_Correlation': partial_corr,\n",
    "            'P_value': partial_p,\n",
    "            'Change': change\n",
    "        })\n",
    "        \n",
    "        print(f\"{x} ‚Üî {y:<25} {raw_corr:>8.3f}   {partial_corr:>8.3f}   {partial_p:>8.3f}   {change:>+6.3f}\")\n",
    "\n",
    "# Visualize partial vs raw correlations\n",
    "partial_df = pd.DataFrame(partial_corr_results).dropna()\n",
    "\n",
    "if not partial_df.empty:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Scatter plot: Raw vs Partial correlations\n",
    "    axes[0].scatter(partial_df['Raw_Correlation'], partial_df['Partial_Correlation'], \n",
    "                   s=100, alpha=0.7, color='purple')\n",
    "    axes[0].plot([-1, 1], [-1, 1], 'r--', alpha=0.5, label='No change line')\n",
    "    axes[0].set_xlabel('Raw Correlation')\n",
    "    axes[0].set_ylabel('Partial Correlation')\n",
    "    axes[0].set_title('Raw vs Partial Correlations')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add labels for points\n",
    "    for i, row in partial_df.iterrows():\n",
    "        axes[0].annotate(f\"{row['Variable_1'][:6]}-{row['Variable_2'][:6]}\", \n",
    "                        (row['Raw_Correlation'], row['Partial_Correlation']),\n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    # Bar plot: Changes in correlation\n",
    "    pair_labels = [f\"{row['Variable_1'][:8]}-{row['Variable_2'][:8]}\" for _, row in partial_df.iterrows()]\n",
    "    colors = ['red' if change < 0 else 'blue' for change in partial_df['Change']]\n",
    "    \n",
    "    axes[1].bar(range(len(partial_df)), partial_df['Change'], color=colors, alpha=0.7)\n",
    "    axes[1].set_xlabel('Variable Pairs')\n",
    "    axes[1].set_ylabel('Change in Correlation')\n",
    "    axes[1].set_title('Change in Correlation After Controlling for Confounders')\n",
    "    axes[1].set_xticks(range(len(partial_df)))\n",
    "    axes[1].set_xticklabels(pair_labels, rotation=45, ha='right')\n",
    "    axes[1].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary\n",
    "    mean_change = partial_df['Change'].mean()\n",
    "    print(f\"\\n‚úÖ Partial correlation analysis completed!\")\n",
    "    print(f\"Average change in correlation after controlling for confounders: {mean_change:+.3f}\")\n",
    "    print(f\"Largest decrease: {partial_df['Change'].min():.3f}\")\n",
    "    print(f\"Largest increase: {partial_df['Change'].max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471c5cd8",
   "metadata": {},
   "source": [
    "## 6. Interactive Correlation Explorer\n",
    "\n",
    "Create an interactive-style correlation explorer with detailed insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf915037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive correlation explorer (static version for notebook)\n",
    "print(\"=== INTERACTIVE CORRELATION EXPLORER ===\")\n",
    "\n",
    "def explore_correlation(feature1, feature2, data):\n",
    "    \"\"\"Detailed correlation analysis between two features\"\"\"\n",
    "    \n",
    "    # Check if features exist\n",
    "    if feature1 not in data.columns or feature2 not in data.columns:\n",
    "        print(f\"‚ùå One or both features not found in dataset\")\n",
    "        return\n",
    "    \n",
    "    # Get clean data\n",
    "    clean_data = data[[feature1, feature2]].dropna()\n",
    "    \n",
    "    if len(clean_data) < 10:\n",
    "        print(f\"‚ùå Insufficient data points ({len(clean_data)}) for analysis\")\n",
    "        return\n",
    "    \n",
    "    # Calculate correlations\n",
    "    pearson_r, pearson_p = pearsonr(clean_data[feature1], clean_data[feature2])\n",
    "    spearman_r, spearman_p = spearmanr(clean_data[feature1], clean_data[feature2])\n",
    "    kendall_r, kendall_p = kendalltau(clean_data[feature1], clean_data[feature2])\n",
    "    \n",
    "    # Create detailed analysis plot\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Scatter plot with regression line\n",
    "    axes[0, 0].scatter(clean_data[feature1], clean_data[feature2], alpha=0.6, s=50)\n",
    "    \n",
    "    # Add regression line\n",
    "    z = np.polyfit(clean_data[feature1], clean_data[feature2], 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[0, 0].plot(clean_data[feature1], p(clean_data[feature1]), \"r--\", alpha=0.8)\n",
    "    \n",
    "    axes[0, 0].set_xlabel(feature1.replace('_', ' ').title())\n",
    "    axes[0, 0].set_ylabel(feature2.replace('_', ' ').title())\n",
    "    axes[0, 0].set_title(f'Scatter Plot: {feature1} vs {feature2}')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add correlation info\n",
    "    axes[0, 0].text(0.05, 0.95, f'Pearson r = {pearson_r:.3f}\\np = {pearson_p:.4f}', \n",
    "                   transform=axes[0, 0].transAxes, verticalalignment='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat'))\n",
    "    \n",
    "    # Distribution plots\n",
    "    axes[0, 1].hist(clean_data[feature1], bins=20, alpha=0.7, color='blue', label=feature1)\n",
    "    axes[0, 1].axvline(clean_data[feature1].mean(), color='blue', linestyle='--', alpha=0.8)\n",
    "    axes[0, 1].set_xlabel(feature1.replace('_', ' ').title())\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].set_title(f'Distribution of {feature1}')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    axes[1, 0].hist(clean_data[feature2], bins=20, alpha=0.7, color='red', label=feature2)\n",
    "    axes[1, 0].axvline(clean_data[feature2].mean(), color='red', linestyle='--', alpha=0.8)\n",
    "    axes[1, 0].set_xlabel(feature2.replace('_', ' ').title())\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title(f'Distribution of {feature2}')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # Correlation comparison table\n",
    "    correlation_data = {\n",
    "        'Method': ['Pearson', 'Spearman', 'Kendall'],\n",
    "        'Correlation': [pearson_r, spearman_r, kendall_r],\n",
    "        'P-value': [pearson_p, spearman_p, kendall_p],\n",
    "        'Significance': ['***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns' \n",
    "                        for p in [pearson_p, spearman_p, kendall_p]]\n",
    "    }\n",
    "    \n",
    "    corr_df = pd.DataFrame(correlation_data)\n",
    "    \n",
    "    # Create table plot\n",
    "    axes[1, 1].axis('tight')\n",
    "    axes[1, 1].axis('off')\n",
    "    table = axes[1, 1].table(cellText=corr_df.round(4).values, colLabels=corr_df.columns,\n",
    "                           cellLoc='center', loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(12)\n",
    "    table.scale(1.2, 2)\n",
    "    axes[1, 1].set_title('Correlation Analysis Results')\n",
    "    \n",
    "    plt.suptitle(f'Detailed Correlation Analysis: {feature1} ‚Üî {feature2}', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed results\n",
    "    print(f\"\\nüìä DETAILED ANALYSIS: {feature1} ‚Üî {feature2}\")\n",
    "    print(f\"Data points: {len(clean_data)}\")\n",
    "    print(f\"Missing values: {len(data) - len(clean_data)}\")\n",
    "    print()\n",
    "    print(\"CORRELATION RESULTS:\")\n",
    "    for _, row in corr_df.iterrows():\n",
    "        strength = \"Very Strong\" if abs(row['Correlation']) > 0.7 else \"Strong\" if abs(row['Correlation']) > 0.5 else \"Moderate\" if abs(row['Correlation']) > 0.3 else \"Weak\"\n",
    "        direction = \"Positive\" if row['Correlation'] > 0 else \"Negative\"\n",
    "        print(f\"‚Ä¢ {row['Method']}: r = {row['Correlation']:.3f} (p = {row['P-value']:.4f}) [{strength} {direction}] {row['Significance']}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"DESCRIPTIVE STATISTICS:\")\n",
    "    print(f\"‚Ä¢ {feature1}: Mean = {clean_data[feature1].mean():.2f}, SD = {clean_data[feature1].std():.2f}\")\n",
    "    print(f\"‚Ä¢ {feature2}: Mean = {clean_data[feature2].mean():.2f}, SD = {clean_data[feature2].std():.2f}\")\n",
    "    \n",
    "    return pearson_r, pearson_p\n",
    "\n",
    "# Demonstrate interactive exploration with key feature pairs\n",
    "interesting_pairs = [\n",
    "    ('nuclear_area', 'nuclear_perimeter'),\n",
    "    ('mitotic_count', 'ki67_index'),\n",
    "    ('chromatin_density', 'chromatin_contrast'),\n",
    "    ('tumor_grade', 'survival_time')\n",
    "]\n",
    "\n",
    "print(\"üîç Exploring interesting feature correlations...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for feature1, feature2 in interesting_pairs:\n",
    "    if feature1 in pathology_data.columns and feature2 in pathology_data.columns:\n",
    "        explore_correlation(feature1, feature2, pathology_data)\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a77f979",
   "metadata": {},
   "source": [
    "## 7. Missing Data Impact on Correlations\n",
    "\n",
    "Analyze how missing data affects correlation estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25871911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing data impact analysis\n",
    "print(\"=== MISSING DATA IMPACT ANALYSIS ===\")\n",
    "\n",
    "# Check current missing data patterns\n",
    "missing_summary = pathology_data.isnull().sum()\n",
    "missing_features = missing_summary[missing_summary > 0]\n",
    "\n",
    "if len(missing_features) > 0:\n",
    "    print(\"Features with missing data:\")\n",
    "    for feature, count in missing_features.items():\n",
    "        percentage = (count / len(pathology_data)) * 100\n",
    "        print(f\"‚Ä¢ {feature}: {count} missing ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Analyze correlation impact\n",
    "    print(\"\\n=== CORRELATION IMPACT ANALYSIS ===\")\n",
    "    \n",
    "    # Compare correlations with different missing data handling strategies\n",
    "    strategies = {\n",
    "        'Listwise Deletion': 'listwise',\n",
    "        'Pairwise Deletion': 'pairwise',\n",
    "        'Mean Imputation': 'mean_impute'\n",
    "    }\n",
    "    \n",
    "    comparison_results = []\n",
    "    \n",
    "    # Select features with some missing data for analysis\n",
    "    test_features = ['nuclear_area', 'ki67_index', 'chromatin_density', 'dab_intensity']\n",
    "    test_features = [f for f in test_features if f in pathology_data.columns]\n",
    "    \n",
    "    for strategy_name, strategy_code in strategies.items():\n",
    "        if strategy_code == 'listwise':\n",
    "            # Use complete cases only\n",
    "            analysis_data = pathology_data[test_features].dropna()\n",
    "            \n",
    "        elif strategy_code == 'pairwise':\n",
    "            # Calculate correlations pairwise (built-in pandas behavior)\n",
    "            analysis_data = pathology_data[test_features]\n",
    "            \n",
    "        elif strategy_code == 'mean_impute':\n",
    "            # Replace missing values with mean\n",
    "            analysis_data = pathology_data[test_features].copy()\n",
    "            for col in analysis_data.columns:\n",
    "                if analysis_data[col].isnull().sum() > 0:\n",
    "                    analysis_data[col].fillna(analysis_data[col].mean(), inplace=True)\n",
    "        \n",
    "        # Calculate correlation matrix\n",
    "        if strategy_code == 'pairwise':\n",
    "            corr_matrix = pathology_data[test_features].corr(method='pearson')\n",
    "        else:\n",
    "            corr_matrix = analysis_data.corr(method='pearson')\n",
    "        \n",
    "        # Store results\n",
    "        comparison_results.append({\n",
    "            'Strategy': strategy_name,\n",
    "            'N_observations': len(analysis_data) if strategy_code != 'pairwise' else 'Varies',\n",
    "            'Mean_correlation': corr_matrix.values[np.triu_indices_from(corr_matrix.values, k=1)].mean(),\n",
    "            'Correlation_matrix': corr_matrix\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n{strategy_name}:\")\n",
    "        print(f\"  Effective sample size: {len(analysis_data) if strategy_code != 'pairwise' else 'Pairwise variable'}\")\n",
    "        print(f\"  Mean absolute correlation: {np.abs(corr_matrix.values[np.triu_indices_from(corr_matrix.values, k=1)]).mean():.3f}\")\n",
    "    \n",
    "    # Visualize comparison\n",
    "    fig, axes = plt.subplots(1, len(comparison_results), figsize=(5 * len(comparison_results), 6))\n",
    "    if len(comparison_results) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, result in enumerate(comparison_results):\n",
    "        sns.heatmap(result['Correlation_matrix'], \n",
    "                   annot=True, \n",
    "                   cmap='RdBu_r', \n",
    "                   center=0,\n",
    "                   vmin=-1, vmax=1,\n",
    "                   square=True,\n",
    "                   ax=axes[i])\n",
    "        axes[i].set_title(f\"{result['Strategy']}\\nN = {result['N_observations']}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical comparison\n",
    "    print(\"\\n=== STRATEGY COMPARISON ===\")\n",
    "    print(\"Differences in correlation estimates:\")\n",
    "    \n",
    "    baseline_corr = comparison_results[0]['Correlation_matrix']  # Listwise as baseline\n",
    "    \n",
    "    for i, result in enumerate(comparison_results[1:], 1):\n",
    "        current_corr = result['Correlation_matrix']\n",
    "        \n",
    "        # Calculate differences\n",
    "        diff_matrix = current_corr - baseline_corr\n",
    "        mean_diff = np.abs(diff_matrix.values[np.triu_indices_from(diff_matrix.values, k=1)]).mean()\n",
    "        max_diff = np.abs(diff_matrix.values).max()\n",
    "        \n",
    "        print(f\"\\n{result['Strategy']} vs Listwise Deletion:\")\n",
    "        print(f\"  Mean absolute difference: {mean_diff:.4f}\")\n",
    "        print(f\"  Maximum absolute difference: {max_diff:.4f}\")\n",
    "        \n",
    "        if mean_diff > 0.1:\n",
    "            print(f\"  ‚ö†Ô∏è  Large differences detected - consider impact on analysis!\")\n",
    "        elif mean_diff > 0.05:\n",
    "            print(f\"  ‚ö° Moderate differences - review carefully\")\n",
    "        else:\n",
    "            print(f\"  ‚úÖ Small differences - strategies are comparable\")\n",
    "\n",
    "else:\n",
    "    print(\"‚úÖ No missing data found in the current dataset\")\n",
    "\n",
    "# Sensitivity analysis - artificially introduce missing data\n",
    "print(\"\\n=== SENSITIVITY ANALYSIS ===\")\n",
    "print(\"Analyzing correlation stability with increasing missing data...\")\n",
    "\n",
    "# Create a copy for sensitivity analysis\n",
    "sensitivity_data = pathology_data.copy()\n",
    "\n",
    "# Select a key correlation pair\n",
    "test_feature1, test_feature2 = 'nuclear_area', 'mitotic_count'\n",
    "if test_feature1 in sensitivity_data.columns and test_feature2 in sensitivity_data.columns:\n",
    "    \n",
    "    # Original correlation\n",
    "    original_corr, _ = pearsonr(sensitivity_data[test_feature1].dropna(), \n",
    "                               sensitivity_data[test_feature2][sensitivity_data[test_feature1].notna()])\n",
    "    \n",
    "    missing_percentages = [0, 5, 10, 15, 20, 25, 30]\n",
    "    correlation_changes = []\n",
    "    \n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    \n",
    "    for missing_pct in missing_percentages:\n",
    "        if missing_pct == 0:\n",
    "            correlation_changes.append(0)\n",
    "            continue\n",
    "            \n",
    "        # Introduce missing data randomly\n",
    "        test_data = sensitivity_data[[test_feature1, test_feature2]].copy()\n",
    "        n_missing = int(len(test_data) * missing_pct / 100)\n",
    "        missing_indices = np.random.choice(test_data.index, n_missing, replace=False)\n",
    "        test_data.loc[missing_indices, test_feature1] = np.nan\n",
    "        \n",
    "        # Calculate correlation with missing data (pairwise deletion)\n",
    "        clean_data = test_data.dropna()\n",
    "        if len(clean_data) > 10:\n",
    "            new_corr, _ = pearsonr(clean_data[test_feature1], clean_data[test_feature2])\n",
    "            correlation_changes.append(abs(new_corr - original_corr))\n",
    "        else:\n",
    "            correlation_changes.append(np.nan)\n",
    "    \n",
    "    # Plot sensitivity analysis\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(missing_percentages, correlation_changes, 'bo-', markersize=8, linewidth=2)\n",
    "    plt.xlabel('Missing Data Percentage (%)')\n",
    "    plt.ylabel('Absolute Change in Correlation')\n",
    "    plt.title(f'Correlation Stability: {test_feature1} ‚Üî {test_feature2}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add threshold lines\n",
    "    plt.axhline(y=0.05, color='orange', linestyle='--', alpha=0.7, label='5% change threshold')\n",
    "    plt.axhline(y=0.1, color='red', linestyle='--', alpha=0.7, label='10% change threshold')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Annotate points\n",
    "    for i, (pct, change) in enumerate(zip(missing_percentages, correlation_changes)):\n",
    "        if not np.isnan(change):\n",
    "            plt.annotate(f'{change:.3f}', (pct, change), textcoords=\"offset points\", \n",
    "                        xytext=(0,10), ha='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Original correlation: {original_corr:.3f}\")\n",
    "    print(\"Stability assessment:\")\n",
    "    for pct, change in zip(missing_percentages[1:], correlation_changes[1:]):\n",
    "        if not np.isnan(change):\n",
    "            stability = \"Stable\" if change < 0.05 else \"Moderate\" if change < 0.1 else \"Unstable\"\n",
    "            print(f\"  {pct:2d}% missing: Œîr = {change:.3f} ({stability})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93324ada",
   "metadata": {},
   "source": [
    "## 8. Auto-Validation Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f17d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-validation tests for correlation and heatmap analysis\n",
    "print(\"=== AUTO-VALIDATION TESTS ===\")\n",
    "\n",
    "# Test 1: Correlation matrix properties\n",
    "correlation_matrix = pathology_data.select_dtypes(include=[np.number]).corr()\n",
    "assert correlation_matrix.shape[0] == correlation_matrix.shape[1], \"‚ùå Correlation matrix not square\"\n",
    "assert np.allclose(np.diag(correlation_matrix), 1.0, atol=1e-10), \"‚ùå Diagonal elements not equal to 1\"\n",
    "assert np.allclose(correlation_matrix.values, correlation_matrix.values.T, atol=1e-10), \"‚ùå Correlation matrix not symmetric\"\n",
    "print(\"‚úÖ Test 1 passed: Correlation matrix properties are correct\")\n",
    "\n",
    "# Test 2: Correlation range validation\n",
    "correlation_values = correlation_matrix.values[np.triu_indices_from(correlation_matrix.values, k=1)]\n",
    "assert np.all((-1 <= correlation_values) & (correlation_values <= 1)), \"‚ùå Correlation values outside valid range\"\n",
    "print(\"‚úÖ Test 2 passed: All correlation values in valid range [-1, 1]\")\n",
    "\n",
    "# Test 3: Statistical significance calculation\n",
    "test_corr, test_p = pearsonr(pathology_data['age'], pathology_data['nuclear_area'])\n",
    "assert 0 <= test_p <= 1, \"‚ùå P-value outside valid range\"\n",
    "assert not np.isnan(test_corr), \"‚ùå Correlation calculation returned NaN\"\n",
    "print(\"‚úÖ Test 3 passed: Statistical calculations are valid\")\n",
    "\n",
    "# Test 4: Missing data handling\n",
    "data_with_missing = pathology_data.copy()\n",
    "data_with_missing.loc[:10, 'nuclear_area'] = np.nan\n",
    "missing_corr_matrix = data_with_missing.corr()\n",
    "assert not missing_corr_matrix.isnull().all().any(), \"‚ùå All correlations are NaN with missing data\"\n",
    "print(\"‚úÖ Test 4 passed: Missing data handled appropriately\")\n",
    "\n",
    "# Test 5: Clustering validation\n",
    "if len(correlation_matrix) > 3:\n",
    "    distance_matrix = 1 - np.abs(correlation_matrix)\n",
    "    linkage_matrix = linkage(squareform(distance_matrix), method='average')\n",
    "    assert linkage_matrix.shape[0] == correlation_matrix.shape[0] - 1, \"‚ùå Incorrect linkage matrix dimensions\"\n",
    "    assert np.all(linkage_matrix[:, 2] >= 0), \"‚ùå Negative distances in linkage matrix\"\n",
    "    print(\"‚úÖ Test 5 passed: Hierarchical clustering works correctly\")\n",
    "\n",
    "# Test 6: Feature grouping validation\n",
    "if 'feature_groups' in locals():\n",
    "    total_features = sum(len(features) for features in feature_groups.values())\n",
    "    assert total_features == len(correlation_matrix.columns), \"‚ùå Feature grouping incomplete\"\n",
    "    print(\"‚úÖ Test 6 passed: Feature grouping is complete\")\n",
    "\n",
    "# Test 7: Heatmap data integrity\n",
    "heatmap_test_data = np.random.rand(5, 5)\n",
    "heatmap_test_data = (heatmap_test_data + heatmap_test_data.T) / 2  # Make symmetric\n",
    "np.fill_diagonal(heatmap_test_data, 1)\n",
    "assert np.allclose(heatmap_test_data, heatmap_test_data.T), \"‚ùå Test heatmap data not symmetric\"\n",
    "print(\"‚úÖ Test 7 passed: Heatmap data structure is valid\")\n",
    "\n",
    "print(\"\\nüéâ All validation tests passed! You've mastered correlation analysis and heatmap visualization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2b4461",
   "metadata": {},
   "source": [
    "## 9. Next Steps\n",
    "\n",
    "Congratulations! You've mastered advanced correlation analysis and heatmap visualization for digital pathology data.\n",
    "\n",
    "**Key Skills Acquired:**\n",
    "‚úÖ Comprehensive correlation matrix analysis  \n",
    "‚úÖ Hierarchical clustering of features  \n",
    "‚úÖ Partial correlation controlling for confounders  \n",
    "‚úÖ Missing data impact assessment  \n",
    "‚úÖ Feature category analysis  \n",
    "‚úÖ Publication-quality heatmap creation  \n",
    "‚úÖ Statistical significance testing  \n",
    "\n",
    "**In the next notebook, you'll learn about:**\n",
    "- UMAP and t-SNE for dimensionality reduction\n",
    "- Interactive visualization techniques\n",
    "- Advanced clustering methods\n",
    "- Multi-dimensional data exploration\n",
    "\n",
    "**For Further Practice:**\n",
    "- Apply these techniques to real TCGA pathology datasets\n",
    "- Explore network analysis of correlated features\n",
    "- Create interactive correlation dashboards\n",
    "- Investigate time-series correlations in longitudinal data\n",
    "\n",
    "**Clinical Applications:**\n",
    "- Biomarker discovery and validation\n",
    "- Feature selection for predictive models\n",
    "- Quality control in pathology workflows\n",
    "- Multi-omics data integration"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
