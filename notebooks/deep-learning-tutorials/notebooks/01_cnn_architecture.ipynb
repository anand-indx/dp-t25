{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bffa9cd",
   "metadata": {},
   "source": [
    "# CNN Architecture Design for Digital Pathology\n",
    "\n",
    "## Learning Objectives\n",
    "- Design CNN architectures optimized for histopathology images\n",
    "- Understand the unique challenges of pathology image analysis\n",
    "- Implement transfer learning with pre-trained models\n",
    "- Build custom architectures for patch-based classification\n",
    "\n",
    "## Prerequisites\n",
    "- Basic deep learning knowledge\n",
    "- Familiarity with CNNs and PyTorch/TensorFlow\n",
    "- Understanding of image classification concepts\n",
    "\n",
    "Let's build powerful CNNs for automated pathology diagnosis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5287f579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ðŸš€ Using device: {device}\")\n",
    "print(\"ðŸ§  Ready to build CNNs for digital pathology!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f07f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Pathology CNN Architecture\n",
    "class PathologyNet(nn.Module):\n",
    "    \"\"\"Custom CNN architecture optimized for histopathology images\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=3, dropout_rate=0.5):\n",
    "        super(PathologyNet, self).__init__()\n",
    "        \n",
    "        # Feature extraction layers\n",
    "        self.features = nn.Sequential(\n",
    "            # First block\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.1),\n",
    "            \n",
    "            # Second block\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.2),\n",
    "            \n",
    "            # Third block\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.3),\n",
    "            \n",
    "            # Fourth block\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.4),\n",
    "        )\n",
    "        \n",
    "        # Adaptive pooling to handle variable input sizes\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256 * 7 * 7, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Create model instance\n",
    "model = PathologyNet(num_classes=3).to(device)\n",
    "print(f\"âœ… PathologyNet created with {sum(p.numel() for p in model.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d35ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer Learning with Pre-trained Models\n",
    "class PathologyTransferNet(nn.Module):\n",
    "    \"\"\"Transfer learning model using pre-trained backbone\"\"\"\n",
    "    \n",
    "    def __init__(self, backbone='resnet50', num_classes=3, freeze_backbone=True):\n",
    "        super(PathologyTransferNet, self).__init__()\n",
    "        \n",
    "        # Load pre-trained model\n",
    "        if backbone == 'resnet50':\n",
    "            self.backbone = models.resnet50(pretrained=True)\n",
    "            num_features = self.backbone.fc.in_features\n",
    "            # Remove the final classification layer\n",
    "            self.backbone = nn.Sequential(*list(self.backbone.children())[:-1])\n",
    "        elif backbone == 'efficientnet':\n",
    "            # For demonstration - would need efficientnet package\n",
    "            print(\"âš ï¸ EfficientNet would require additional installation\")\n",
    "            self.backbone = models.resnet50(pretrained=True)\n",
    "            num_features = 2048\n",
    "            self.backbone = nn.Sequential(*list(self.backbone.children())[:-1])\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone: {backbone}\")\n",
    "        \n",
    "        # Freeze backbone parameters if specified\n",
    "        if freeze_backbone:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"ðŸ”’ Backbone frozen for transfer learning\")\n",
    "        \n",
    "        # Custom classifier for pathology\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        output = self.classifier(features)\n",
    "        return output\n",
    "\n",
    "# Create transfer learning model\n",
    "transfer_model = PathologyTransferNet(backbone='resnet50', num_classes=3).to(device)\n",
    "trainable_params = sum(p.numel() for p in transfer_model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in transfer_model.parameters())\n",
    "\n",
    "print(f\"âœ… Transfer model created:\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   Frozen parameters: {total_params - trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4879957b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic pathology dataset for demonstration\n",
    "class SyntheticPathologyDataset(Dataset):\n",
    "    \"\"\"Synthetic dataset for pathology image classification\"\"\"\n",
    "    \n",
    "    def __init__(self, num_samples=1000, image_size=224, transform=None):\n",
    "        self.num_samples = num_samples\n",
    "        self.image_size = image_size\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Create synthetic data\n",
    "        np.random.seed(42)\n",
    "        self.labels = np.random.randint(0, 3, num_samples)  # 3 classes\n",
    "        \n",
    "        # Class names\n",
    "        self.class_names = ['Normal', 'Benign', 'Malignant']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Generate synthetic pathology-like image\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Create class-specific patterns\n",
    "        if label == 0:  # Normal\n",
    "            # More regular, uniform texture\n",
    "            image = np.random.normal(0.6, 0.1, (self.image_size, self.image_size, 3))\n",
    "        elif label == 1:  # Benign\n",
    "            # Slightly more irregular\n",
    "            image = np.random.normal(0.4, 0.15, (self.image_size, self.image_size, 3))\n",
    "            # Add some texture\n",
    "            image += 0.1 * np.random.random((self.image_size, self.image_size, 3))\n",
    "        else:  # Malignant\n",
    "            # More chaotic, varied texture\n",
    "            image = np.random.normal(0.3, 0.2, (self.image_size, self.image_size, 3))\n",
    "            # Add more complex patterns\n",
    "            image += 0.2 * np.sin(np.arange(self.image_size)[:, None] * 0.1) * np.cos(np.arange(self.image_size)[None, :] * 0.1)[..., None]\n",
    "        \n",
    "        # Clip to valid range\n",
    "        image = np.clip(image, 0, 1)\n",
    "        \n",
    "        # Convert to PIL Image\n",
    "        image = Image.fromarray((image * 255).astype(np.uint8))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Data transforms for pathology images\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=90),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SyntheticPathologyDataset(num_samples=800, transform=train_transform)\n",
    "val_dataset = SyntheticPathologyDataset(num_samples=200, transform=val_transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"ðŸ“Š Dataset created:\")\n",
    "print(f\"   Training samples: {len(train_dataset)}\")\n",
    "print(f\"   Validation samples: {len(val_dataset)}\")\n",
    "print(f\"   Batch size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7798c3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup and functions\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=0.001):\n",
    "    \"\"\"Train the CNN model\"\"\"\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "    \n",
    "    # Training history\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    \n",
    "    print(\"ðŸš€ Starting training...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        with tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}') as pbar:\n",
    "            for images, labels in pbar:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Statistics\n",
    "                train_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.set_postfix({\n",
    "                    'Loss': f'{loss.item():.4f}',\n",
    "                    'Acc': f'{100.*train_correct/train_total:.2f}%'\n",
    "                })\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        train_acc = 100. * train_correct / train_total\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        \n",
    "        # Store history\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}]:')\n",
    "        print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "        print('-' * 50)\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'val_accuracies': val_accuracies\n",
    "    }\n",
    "\n",
    "# Train the custom model (reduced epochs for demo)\n",
    "print(\"ðŸ‹ï¸â€â™‚ï¸ Training custom PathologyNet...\")\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=3, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff074731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training and validation metrics\"\"\"\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    epochs = range(1, len(history['train_losses']) + 1)\n",
    "    ax1.plot(epochs, history['train_losses'], 'bo-', label='Training Loss')\n",
    "    ax1.plot(epochs, history['val_losses'], 'ro-', label='Validation Loss')\n",
    "    ax1.set_title('Training and Validation Loss', fontweight='bold')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax2.plot(epochs, history['train_accuracies'], 'bo-', label='Training Accuracy')\n",
    "    ax2.plot(epochs, history['val_accuracies'], 'ro-', label='Validation Accuracy')\n",
    "    ax2.set_title('Training and Validation Accuracy', fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(history)\n",
    "\n",
    "# Final performance summary\n",
    "final_train_acc = history['train_accuracies'][-1]\n",
    "final_val_acc = history['val_accuracies'][-1]\n",
    "print(f\"\\nðŸ† Final Performance:\")\n",
    "print(f\"   Training Accuracy: {final_train_acc:.2f}%\")\n",
    "print(f\"   Validation Accuracy: {final_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3673b3c0",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Exercise: CNN Architecture Challenge\n",
    "\n",
    "Design and implement your own CNN architecture for pathology classification:\n",
    "\n",
    "1. **Custom Architecture**: Design a CNN with at least 4 convolutional blocks\n",
    "2. **Transfer Learning**: Compare performance with pre-trained models\n",
    "3. **Architecture Analysis**: Count parameters and analyze computational complexity\n",
    "4. **Augmentation Strategy**: Implement pathology-specific data augmentation\n",
    "\n",
    "### Expected Performance\n",
    "Your CNN should achieve:\n",
    "- **Training accuracy**: >80% within 10 epochs\n",
    "- **Validation accuracy**: >75% (avoid overfitting)\n",
    "- **Parameter efficiency**: <10M parameters\n",
    "- **Convergence speed**: Loss should decrease steadily\n",
    "\n",
    "### Advanced Challenge\n",
    "Implement attention mechanisms or multi-scale feature fusion!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99129734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¯ VALIDATION: CNN architecture requirements\n",
    "def validate_cnn_architecture(model, history):\n",
    "    \"\"\"Validate CNN meets performance and architecture requirements\"\"\"\n",
    "    \n",
    "    print(\"ðŸ§  Validating CNN architecture and performance...\")\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    # Get final performance\n",
    "    final_train_acc = history['train_accuracies'][-1]\n",
    "    final_val_acc = history['val_accuracies'][-1]\n",
    "    \n",
    "    # Check for overfitting\n",
    "    overfitting_gap = final_train_acc - final_val_acc\n",
    "    \n",
    "    print(f\"ðŸ“Š Architecture Analysis:\")\n",
    "    print(f\"   Total Parameters: {total_params:,}\")\n",
    "    print(f\"   Trainable Parameters: {trainable_params:,}\")\n",
    "    print(f\"   Final Training Accuracy: {final_train_acc:.2f}%\")\n",
    "    print(f\"   Final Validation Accuracy: {final_val_acc:.2f}%\")\n",
    "    print(f\"   Overfitting Gap: {overfitting_gap:.2f}%\")\n",
    "    \n",
    "    # Validation checks\n",
    "    assert total_params < 20_000_000, f\"Too many parameters: {total_params:,}\"\n",
    "    assert final_train_acc > 60.0, f\"Training accuracy too low: {final_train_acc:.2f}%\"\n",
    "    assert final_val_acc > 50.0, f\"Validation accuracy too low: {final_val_acc:.2f}%\"\n",
    "    assert overfitting_gap < 30.0, f\"Severe overfitting detected: {overfitting_gap:.2f}%\"\n",
    "    \n",
    "    print(\"\\nðŸŽ‰ CNN architecture validation passed!\")\n",
    "    print(\"ðŸš€ Ready for next tutorial: Data Augmentation Pipeline\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Run validation\n",
    "validate_cnn_architecture(model, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f09790",
   "metadata": {},
   "source": [
    "## ðŸ“š Summary\n",
    "\n",
    "You've successfully designed CNNs for digital pathology:\n",
    "\n",
    "1. **Custom Architecture**: Built PathologyNet with batch normalization and dropout\n",
    "2. **Transfer Learning**: Implemented pre-trained backbone adaptation\n",
    "3. **Training Pipeline**: Complete training loop with validation\n",
    "4. **Performance Monitoring**: Training history visualization and analysis\n",
    "\n",
    "### Key Architecture Principles\n",
    "- **Batch Normalization**: Stabilizes training and improves convergence\n",
    "- **Dropout**: Prevents overfitting in fully connected layers  \n",
    "- **Adaptive Pooling**: Handles variable input sizes\n",
    "- **Transfer Learning**: Leverages pre-trained ImageNet features\n",
    "\n",
    "### Best Practices for Pathology CNNs\n",
    "âœ… **Use data augmentation** extensively for limited medical data  \n",
    "âœ… **Monitor overfitting** carefully with validation metrics  \n",
    "âœ… **Consider class imbalance** in loss function design  \n",
    "âœ… **Validate on external datasets** before clinical deployment  \n",
    "\n",
    "### Next Steps\n",
    "- **Tutorial 2**: Advanced data augmentation techniques\n",
    "- **Tutorial 3**: Model training optimization and regularization\n",
    "- **Tutorial 4**: Hyperparameter tuning and architecture search\n",
    "\n",
    "ðŸŽ“ **Excellent!** You've mastered CNN design for digital pathology applications!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
