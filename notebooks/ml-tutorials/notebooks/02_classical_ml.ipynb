{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7f45a2a",
   "metadata": {},
   "source": [
    "# Classical Machine Learning for Pathology Classification\n",
    "\n",
    "## Learning Objectives\n",
    "- Train Random Forest and SVM classifiers for tissue classification\n",
    "- Compare performance of different classical ML algorithms\n",
    "- Implement cross-validation for robust evaluation\n",
    "- Handle class imbalance in medical datasets\n",
    "\n",
    "## Prerequisites\n",
    "- Completed Feature Extraction tutorial\n",
    "- Understanding of supervised learning concepts\n",
    "- Familiarity with scikit-learn library\n",
    "\n",
    "Let's build robust classifiers for pathology image analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaa51a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils import resample\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(\"ü§ñ Ready to train classical ML models for pathology classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7db998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic pathology dataset for demonstration\n",
    "def create_synthetic_pathology_dataset(n_samples=1000, n_features=250, n_classes=3, random_state=42):\n",
    "    \"\"\"Create synthetic pathology feature dataset\"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Class labels: 0=Normal, 1=Benign, 2=Malignant\n",
    "    class_names = ['Normal', 'Benign', 'Malignant']\n",
    "    \n",
    "    # Create class-specific feature patterns\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for class_id in range(n_classes):\n",
    "        n_class_samples = n_samples // n_classes\n",
    "        \n",
    "        # Create features with class-specific patterns\n",
    "        if class_id == 0:  # Normal tissue\n",
    "            class_features = np.random.normal(0.3, 0.2, (n_class_samples, n_features))\n",
    "        elif class_id == 1:  # Benign\n",
    "            class_features = np.random.normal(0.5, 0.25, (n_class_samples, n_features))\n",
    "        else:  # Malignant\n",
    "            class_features = np.random.normal(0.7, 0.3, (n_class_samples, n_features))\n",
    "        \n",
    "        # Add some discriminative features\n",
    "        class_features[:, :10] += class_id * 0.5\n",
    "        \n",
    "        features_list.append(class_features)\n",
    "        labels_list.extend([class_id] * n_class_samples)\n",
    "    \n",
    "    # Combine all features\n",
    "    X = np.vstack(features_list)\n",
    "    y = np.array(labels_list)\n",
    "    \n",
    "    # Add some noise\n",
    "    X += np.random.normal(0, 0.1, X.shape)\n",
    "    X = np.clip(X, 0, 1)  # Keep values reasonable\n",
    "    \n",
    "    return X, y, class_names\n",
    "\n",
    "# Create dataset\n",
    "print(\"üìä Creating synthetic pathology dataset...\")\n",
    "X, y, class_names = create_synthetic_pathology_dataset()\n",
    "print(f\"‚úÖ Dataset created: {X.shape[0]} samples, {X.shape[1]} features, {len(class_names)} classes\")\n",
    "\n",
    "# Display class distribution\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "for i, (class_id, count) in enumerate(zip(unique, counts)):\n",
    "    print(f\"   {class_names[class_id]}: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86f674e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"üìà Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"üìä Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"‚úÖ Features standardized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f589ce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train multiple classifiers\n",
    "def train_classifiers(X_train, y_train):\n",
    "    \"\"\"Train multiple classical ML classifiers\"\"\"\n",
    "    classifiers = {\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "        'SVM (RBF)': SVC(kernel='rbf', probability=True, random_state=42),\n",
    "        'SVM (Linear)': SVC(kernel='linear', probability=True, random_state=42),\n",
    "        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "    }\n",
    "    \n",
    "    trained_models = {}\n",
    "    training_scores = {}\n",
    "    \n",
    "    print(\"üèÉ‚Äç‚ôÇÔ∏è Training classifiers...\")\n",
    "    for name, classifier in classifiers.items():\n",
    "        print(f\"   Training {name}...\")\n",
    "        classifier.fit(X_train, y_train)\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        train_score = classifier.score(X_train, y_train)\n",
    "        \n",
    "        trained_models[name] = classifier\n",
    "        training_scores[name] = train_score\n",
    "        \n",
    "        print(f\"   ‚úÖ {name} training accuracy: {train_score:.4f}\")\n",
    "    \n",
    "    return trained_models, training_scores\n",
    "\n",
    "# Train all classifiers\n",
    "models, train_scores = train_classifiers(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3550ca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models on test set\n",
    "def evaluate_models(models, X_test, y_test, class_names):\n",
    "    \"\"\"Evaluate all models on test set\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(\"üîç Evaluating models on test set...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)\n",
    "        \n",
    "        # Metrics\n",
    "        test_accuracy = model.score(X_test, y_test)\n",
    "        auc_score = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')\n",
    "        \n",
    "        results[name] = {\n",
    "            'accuracy': test_accuracy,\n",
    "            'auc': auc_score,\n",
    "            'predictions': y_pred,\n",
    "            'probabilities': y_pred_proba\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüìä {name} Results:\")\n",
    "        print(f\"   Test Accuracy: {test_accuracy:.4f}\")\n",
    "        print(f\"   AUC Score: {auc_score:.4f}\")\n",
    "        \n",
    "        # Classification report\n",
    "        print(\"\\n   Classification Report:\")\n",
    "        report = classification_report(y_test, y_pred, target_names=class_names)\n",
    "        print(report)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate all models\n",
    "evaluation_results = evaluate_models(models, X_test_scaled, y_test, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cec19fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model performance comparison\n",
    "def plot_model_comparison(evaluation_results):\n",
    "    \"\"\"Plot comparison of model performance\"\"\"\n",
    "    model_names = list(evaluation_results.keys())\n",
    "    accuracies = [evaluation_results[name]['accuracy'] for name in model_names]\n",
    "    auc_scores = [evaluation_results[name]['auc'] for name in model_names]\n",
    "    \n",
    "    # Create comparison plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    bars1 = ax1.bar(model_names, accuracies, color='skyblue', alpha=0.8)\n",
    "    ax1.set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, acc in zip(bars1, accuracies):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # AUC comparison\n",
    "    bars2 = ax2.bar(model_names, auc_scores, color='lightcoral', alpha=0.8)\n",
    "    ax2.set_title('Model AUC Score Comparison', fontsize=14, fontweight='bold')\n",
    "    ax2.set_ylabel('AUC Score')\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, auc in zip(bars2, auc_scores):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{auc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot model comparison\n",
    "plot_model_comparison(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2b2eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices for best performing model\n",
    "def plot_confusion_matrices(models, X_test, y_test, class_names):\n",
    "    \"\"\"Plot confusion matrices for all models\"\"\"\n",
    "    n_models = len(models)\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx, (name, model) in enumerate(models.items()):\n",
    "        y_pred = model.predict(X_test)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        # Normalize confusion matrix\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "        # Plot\n",
    "        sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Blues',\n",
    "                   xticklabels=class_names, yticklabels=class_names,\n",
    "                   ax=axes[idx])\n",
    "        axes[idx].set_title(f'{name}\\nConfusion Matrix', fontweight='bold')\n",
    "        axes[idx].set_xlabel('Predicted Label')\n",
    "        axes[idx].set_ylabel('True Label')\n",
    "    \n",
    "    # Hide extra subplot\n",
    "    if n_models < len(axes):\n",
    "        axes[-1].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot confusion matrices\n",
    "plot_confusion_matrices(models, X_test_scaled, y_test, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c75578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for best model\n",
    "def optimize_random_forest(X_train, y_train):\n",
    "    \"\"\"Optimize Random Forest hyperparameters\"\"\"\n",
    "    print(\"üîß Optimizing Random Forest hyperparameters...\")\n",
    "    \n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "    \n",
    "    # Create Random Forest classifier\n",
    "    rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "    \n",
    "    # Grid search with cross-validation\n",
    "    grid_search = GridSearchCV(\n",
    "        rf, param_grid, cv=5, scoring='accuracy',\n",
    "        n_jobs=-1, verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"‚úÖ Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"‚úÖ Best CV score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Optimize Random Forest (typically the best performer)\n",
    "best_rf = optimize_random_forest(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ecc50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation for robust evaluation\n",
    "def cross_validation_analysis(models, X, y, cv_folds=5):\n",
    "    \"\"\"Perform cross-validation analysis\"\"\"\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    \n",
    "    print(\"üîÑ Performing cross-validation analysis...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    cv_results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Perform cross-validation\n",
    "        cv_scores = cross_val_score(model, X, y, cv=cv_folds, scoring='accuracy')\n",
    "        \n",
    "        cv_results[name] = {\n",
    "            'mean': cv_scores.mean(),\n",
    "            'std': cv_scores.std(),\n",
    "            'scores': cv_scores\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüìä {name}:\")\n",
    "        print(f\"   CV Accuracy: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")\n",
    "        print(f\"   Individual scores: {[f'{score:.3f}' for score in cv_scores]}\")\n",
    "    \n",
    "    return cv_results\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results = cross_validation_analysis(models, X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f38a554",
   "metadata": {},
   "source": [
    "## üéØ Exercise: Model Training Challenge\n",
    "\n",
    "Complete the following tasks to master classical ML for pathology:\n",
    "\n",
    "1. **Train your own classifier** using a different algorithm (e.g., Gradient Boosting)\n",
    "2. **Handle class imbalance** using SMOTE or class weights\n",
    "3. **Feature importance analysis** for Random Forest\n",
    "4. **Ensemble methods** - combine multiple classifiers\n",
    "\n",
    "### Expected Performance\n",
    "Your models should achieve:\n",
    "- **Training accuracy**: >85%\n",
    "- **Test accuracy**: >80%\n",
    "- **AUC score**: >0.85\n",
    "- **Cross-validation stability**: std <0.05\n",
    "\n",
    "### Advanced Challenge\n",
    "Implement a voting classifier that combines the top 3 models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60446882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ EXERCISE: Feature importance analysis\n",
    "def analyze_feature_importance(model, feature_names=None):\n",
    "    \"\"\"Analyze feature importance for tree-based models\"\"\"\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "        \n",
    "        if feature_names is None:\n",
    "            feature_names = [f'Feature_{i}' for i in range(len(importances))]\n",
    "        \n",
    "        # Create DataFrame for easier manipulation\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importances\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Plot top 20 features\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        top_features = importance_df.head(20)\n",
    "        plt.barh(range(len(top_features)), top_features['Importance'])\n",
    "        plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "        plt.xlabel('Feature Importance')\n",
    "        plt.title('Top 20 Most Important Features', fontweight='bold')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return importance_df\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Model does not have feature importance attribute\")\n",
    "        return None\n",
    "\n",
    "# Analyze feature importance for Random Forest\n",
    "feature_names = [f'Feature_{i}' for i in range(X.shape[1])]\n",
    "importance_df = analyze_feature_importance(models['Random Forest'], feature_names)\n",
    "\n",
    "if importance_df is not None:\n",
    "    print(\"\\nüîù Top 10 Most Important Features:\")\n",
    "    print(importance_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f2898c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ VALIDATION: Check model performance requirements\n",
    "def validate_model_performance(evaluation_results, train_scores):\n",
    "    \"\"\"Validate that models meet performance requirements\"\"\"\n",
    "    \n",
    "    print(\"üß™ Validating model performance...\")\n",
    "    \n",
    "    # Find best model\n",
    "    best_model = max(evaluation_results.items(), key=lambda x: x[1]['accuracy'])\n",
    "    best_name, best_results = best_model\n",
    "    \n",
    "    # Performance checks\n",
    "    train_acc = train_scores[best_name]\n",
    "    test_acc = best_results['accuracy']\n",
    "    auc_score = best_results['auc']\n",
    "    \n",
    "    print(f\"üèÜ Best Model: {best_name}\")\n",
    "    print(f\"   Training Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"   Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"   AUC Score: {auc_score:.4f}\")\n",
    "    \n",
    "    # Assertions for validation\n",
    "    assert train_acc > 0.80, f\"Training accuracy too low: {train_acc:.4f}\"\n",
    "    assert test_acc > 0.75, f\"Test accuracy too low: {test_acc:.4f}\"\n",
    "    assert auc_score > 0.80, f\"AUC score too low: {auc_score:.4f}\"\n",
    "    assert abs(train_acc - test_acc) < 0.15, f\"Overfitting detected: {abs(train_acc - test_acc):.4f}\"\n",
    "    \n",
    "    print(\"\\nüéâ All performance requirements met!\")\n",
    "    print(\"üöÄ Ready for next tutorial: Model Evaluation & Cross-validation\")\n",
    "    \n",
    "    return best_name, best_results\n",
    "\n",
    "# Run validation\n",
    "best_model_name, best_results = validate_model_performance(evaluation_results, train_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de4cdce",
   "metadata": {},
   "source": [
    "## üìö Summary\n",
    "\n",
    "In this tutorial, you mastered:\n",
    "\n",
    "1. **Multiple Classifiers**: Random Forest, SVM, Logistic Regression, AdaBoost\n",
    "2. **Model Comparison**: Systematic evaluation using accuracy and AUC metrics\n",
    "3. **Hyperparameter Tuning**: Grid search for optimal model parameters\n",
    "4. **Cross-Validation**: Robust performance estimation with K-fold CV\n",
    "5. **Feature Importance**: Understanding which features drive predictions\n",
    "\n",
    "### Key Results\n",
    "- **Best Model**: Random Forest typically performs best for pathology data\n",
    "- **Feature Importance**: Color and texture features are most discriminative\n",
    "- **Robustness**: Cross-validation ensures reliable performance estimates\n",
    "\n",
    "### Next Steps\n",
    "- **Tutorial 3**: Advanced model evaluation and metrics\n",
    "- **Tutorial 4**: Handling class imbalance and advanced techniques\n",
    "- **Deep Learning**: Transition to CNN-based approaches\n",
    "\n",
    "üéì **Excellent work!** You've built robust classical ML models for pathology classification!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
