{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55e61a25",
   "metadata": {},
   "source": [
    "# Advanced Model Evaluation for Pathology Classification\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement comprehensive evaluation metrics for medical AI\n",
    "- Handle class imbalance with appropriate metrics\n",
    "- Create ROC curves and precision-recall curves\n",
    "- Perform statistical significance testing\n",
    "- Generate detailed performance reports\n",
    "\n",
    "## Prerequisites\n",
    "- Completed Classical ML tutorial\n",
    "- Understanding of classification metrics\n",
    "- Familiarity with medical AI evaluation standards\n",
    "\n",
    "Let's master advanced evaluation techniques for pathology AI!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135904c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_curve, auc,\n",
    "    precision_recall_curve, average_precision_score, matthews_corrcoef,\n",
    "    balanced_accuracy_score, cohen_kappa_score\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold, permutation_test_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import scipy.stats as stats\n",
    "from itertools import cycle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")\n",
    "print(\"📊 Ready for advanced model evaluation in pathology!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c9bc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate realistic pathology evaluation scenario\n",
    "def create_evaluation_dataset():\n",
    "    \"\"\"Create realistic pathology dataset with class imbalance\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Realistic class distribution in pathology\n",
    "    class_sizes = [600, 300, 100]  # Normal, Benign, Malignant\n",
    "    class_names = ['Normal', 'Benign', 'Malignant']\n",
    "    \n",
    "    X_list, y_list = [], []\n",
    "    \n",
    "    for class_id, n_samples in enumerate(class_sizes):\n",
    "        # Create class-specific feature distributions\n",
    "        if class_id == 0:  # Normal\n",
    "            features = np.random.multivariate_normal(\n",
    "                mean=[0.2, 0.3, 0.1], cov=[[0.1, 0.02, 0.01], [0.02, 0.1, 0.01], [0.01, 0.01, 0.1]],\n",
    "                size=(n_samples, 100)\n",
    "            ).reshape(n_samples, -1)\n",
    "        elif class_id == 1:  # Benign\n",
    "            features = np.random.multivariate_normal(\n",
    "                mean=[0.5, 0.4, 0.3], cov=[[0.15, 0.03, 0.02], [0.03, 0.15, 0.02], [0.02, 0.02, 0.15]],\n",
    "                size=(n_samples, 100)\n",
    "            ).reshape(n_samples, -1)\n",
    "        else:  # Malignant\n",
    "            features = np.random.multivariate_normal(\n",
    "                mean=[0.8, 0.7, 0.6], cov=[[0.2, 0.04, 0.03], [0.04, 0.2, 0.03], [0.03, 0.03, 0.2]],\n",
    "                size=(n_samples, 100)\n",
    "            ).reshape(n_samples, -1)\n",
    "        \n",
    "        X_list.append(features)\n",
    "        y_list.extend([class_id] * n_samples)\n",
    "    \n",
    "    X = np.vstack(X_list)\n",
    "    y = np.array(y_list)\n",
    "    \n",
    "    return X, y, class_names\n",
    "\n",
    "# Create evaluation dataset\n",
    "X_eval, y_eval, class_names = create_evaluation_dataset()\n",
    "print(f\"📊 Evaluation dataset: {X_eval.shape[0]} samples, {X_eval.shape[1]} features\")\n",
    "\n",
    "# Show class distribution\n",
    "unique, counts = np.unique(y_eval, return_counts=True)\n",
    "for class_id, count in zip(unique, counts):\n",
    "    print(f\"   {class_names[class_id]}: {count} samples ({count/len(y_eval)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca98316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline model for evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_eval, y_eval, test_size=0.3, random_state=42, stratify=y_eval\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train Random Forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get predictions and probabilities\n",
    "y_pred = rf_model.predict(X_test_scaled)\n",
    "y_pred_proba = rf_model.predict_proba(X_test_scaled)\n",
    "\n",
    "print(\"✅ Baseline model trained and predictions generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70c2a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive evaluation metrics\n",
    "def comprehensive_evaluation(y_true, y_pred, y_pred_proba, class_names):\n",
    "    \"\"\"Generate comprehensive evaluation report\"\"\"\n",
    "    \n",
    "    print(\"📊 COMPREHENSIVE MODEL EVALUATION REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Basic metrics\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    kappa = cohen_kappa_score(y_true, y_pred)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\n🎯 OVERALL PERFORMANCE:\")\n",
    "    print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   Balanced Accuracy: {balanced_acc:.4f}\")\n",
    "    print(f\"   Cohen's Kappa: {kappa:.4f}\")\n",
    "    print(f\"   Matthews Correlation Coefficient: {mcc:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(f\"\\n📋 DETAILED CLASSIFICATION REPORT:\")\n",
    "    report = classification_report(y_true, y_pred, target_names=class_names)\n",
    "    print(report)\n",
    "    \n",
    "    # Per-class metrics\n",
    "    print(f\"\\n🎯 PER-CLASS PERFORMANCE:\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    for i, class_name in enumerate(class_names):\n",
    "        TP = cm[i, i]\n",
    "        FP = np.sum(cm[:, i]) - TP\n",
    "        FN = np.sum(cm[i, :]) - TP\n",
    "        TN = np.sum(cm) - TP - FP - FN\n",
    "        \n",
    "        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "        ppv = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        npv = TN / (TN + FN) if (TN + FN) > 0 else 0\n",
    "        \n",
    "        print(f\"\\n   {class_name}:\")\n",
    "        print(f\"      Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "        print(f\"      Specificity: {specificity:.4f}\")\n",
    "        print(f\"      PPV (Precision): {ppv:.4f}\")\n",
    "        print(f\"      NPV: {npv:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'balanced_accuracy': balanced_acc,\n",
    "        'kappa': kappa,\n",
    "        'mcc': mcc,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "# Generate comprehensive evaluation\n",
    "eval_results = comprehensive_evaluation(y_test, y_pred, y_pred_proba, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80704b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves for multi-class classification\n",
    "def plot_multiclass_roc_curves(y_true, y_pred_proba, class_names):\n",
    "    \"\"\"Plot ROC curves for each class in multi-class setting\"\"\"\n",
    "    \n",
    "    # Binarize labels for ROC analysis\n",
    "    y_bin = label_binarize(y_true, classes=[0, 1, 2])\n",
    "    n_classes = len(class_names)\n",
    "    \n",
    "    # Compute ROC curve and AUC for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], y_pred_proba[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # Compute micro-average ROC curve\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_bin.ravel(), y_pred_proba.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    colors = cycle(['blue', 'red', 'green', 'orange', 'purple'])\n",
    "    \n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                label=f'{class_names[i]} (AUC = {roc_auc[i]:.3f})')\n",
    "    \n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"], color='gold', lw=2, linestyle='--',\n",
    "            label=f'Micro-average (AUC = {roc_auc[\"micro\"]:.3f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Multi-class ROC Curves', fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    return roc_auc\n",
    "\n",
    "# Plot ROC curves\n",
    "roc_scores = plot_multiclass_roc_curves(y_test, y_pred_proba, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78102cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall curves\n",
    "def plot_precision_recall_curves(y_true, y_pred_proba, class_names):\n",
    "    \"\"\"Plot Precision-Recall curves for each class\"\"\"\n",
    "    \n",
    "    y_bin = label_binarize(y_true, classes=[0, 1, 2])\n",
    "    n_classes = len(class_names)\n",
    "    \n",
    "    # Compute PR curve for each class\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    avg_precision = dict()\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    colors = cycle(['blue', 'red', 'green', 'orange', 'purple'])\n",
    "    \n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_bin[:, i], y_pred_proba[:, i])\n",
    "        avg_precision[i] = average_precision_score(y_bin[:, i], y_pred_proba[:, i])\n",
    "        \n",
    "        plt.plot(recall[i], precision[i], color=color, lw=2,\n",
    "                label=f'{class_names[i]} (AP = {avg_precision[i]:.3f})')\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Multi-class Precision-Recall Curves', fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    return avg_precision\n",
    "\n",
    "# Plot Precision-Recall curves\n",
    "pr_scores = plot_precision_recall_curves(y_test, y_pred_proba, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e5d5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced confusion matrix visualization\n",
    "def plot_enhanced_confusion_matrix(y_true, y_pred, class_names):\n",
    "    \"\"\"Create enhanced confusion matrix with percentages\"\"\"\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Raw counts\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names, ax=ax1)\n",
    "    ax1.set_title('Confusion Matrix (Counts)', fontweight='bold')\n",
    "    ax1.set_xlabel('Predicted Label')\n",
    "    ax1.set_ylabel('True Label')\n",
    "    \n",
    "    # Percentages\n",
    "    sns.heatmap(cm_percent, annot=True, fmt='.1f', cmap='Oranges',\n",
    "                xticklabels=class_names, yticklabels=class_names, ax=ax2)\n",
    "    ax2.set_title('Confusion Matrix (Percentages)', fontweight='bold')\n",
    "    ax2.set_xlabel('Predicted Label')\n",
    "    ax2.set_ylabel('True Label')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate per-class accuracies\n",
    "    class_accuracies = cm.diagonal() / cm.sum(axis=1)\n",
    "    print(\"\\n🎯 Per-class Accuracies:\")\n",
    "    for i, (class_name, acc) in enumerate(zip(class_names, class_accuracies)):\n",
    "        print(f\"   {class_name}: {acc:.3f} ({acc*100:.1f}%)\")\n",
    "\n",
    "# Plot enhanced confusion matrix\n",
    "plot_enhanced_confusion_matrix(y_test, y_pred, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae6c31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical significance testing\n",
    "def statistical_significance_test(model, X, y, n_permutations=100):\n",
    "    \"\"\"Perform permutation test for statistical significance\"\"\"\n",
    "    \n",
    "    print(\"🧪 Performing statistical significance test...\")\n",
    "    \n",
    "    # Permutation test\n",
    "    score, perm_scores, pvalue = permutation_test_score(\n",
    "        model, X, y, scoring=\"accuracy\", cv=5, n_permutations=n_permutations, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"   Model Accuracy: {score:.4f}\")\n",
    "    print(f\"   Permutation Test p-value: {pvalue:.6f}\")\n",
    "    \n",
    "    if pvalue < 0.05:\n",
    "        print(\"   ✅ Model performance is statistically significant!\")\n",
    "    else:\n",
    "        print(\"   ⚠️ Model performance is NOT statistically significant\")\n",
    "    \n",
    "    # Plot permutation scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(perm_scores, bins=20, alpha=0.7, color='lightblue', edgecolor='black')\n",
    "    plt.axvline(score, color='red', linestyle='--', linewidth=2, label=f'Model Score: {score:.3f}')\n",
    "    plt.xlabel('Accuracy Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Permutation Test Results', fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    return score, pvalue\n",
    "\n",
    "# Perform significance test\n",
    "model_score, p_value = statistical_significance_test(rf_model, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147456c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap confidence intervals\n",
    "def bootstrap_confidence_intervals(y_true, y_pred, n_bootstrap=1000, confidence=0.95):\n",
    "    \"\"\"Calculate bootstrap confidence intervals for performance metrics\"\"\"\n",
    "    \n",
    "    print(f\"🔄 Calculating {confidence*100}% confidence intervals...\")\n",
    "    \n",
    "    n_samples = len(y_true)\n",
    "    bootstrap_accuracies = []\n",
    "    \n",
    "    # Bootstrap sampling\n",
    "    for _ in range(n_bootstrap):\n",
    "        # Sample with replacement\n",
    "        indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "        y_true_boot = y_true[indices]\n",
    "        y_pred_boot = y_pred[indices]\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = np.mean(y_true_boot == y_pred_boot)\n",
    "        bootstrap_accuracies.append(accuracy)\n",
    "    \n",
    "    # Calculate confidence intervals\n",
    "    alpha = 1 - confidence\n",
    "    lower = np.percentile(bootstrap_accuracies, (alpha/2) * 100)\n",
    "    upper = np.percentile(bootstrap_accuracies, (1 - alpha/2) * 100)\n",
    "    mean_acc = np.mean(bootstrap_accuracies)\n",
    "    \n",
    "    print(f\"   Mean Accuracy: {mean_acc:.4f}\")\n",
    "    print(f\"   {confidence*100}% CI: [{lower:.4f}, {upper:.4f}]\")\n",
    "    print(f\"   CI Width: {upper - lower:.4f}\")\n",
    "    \n",
    "    # Plot bootstrap distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(bootstrap_accuracies, bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    plt.axvline(mean_acc, color='blue', linestyle='-', linewidth=2, label=f'Mean: {mean_acc:.3f}')\n",
    "    plt.axvline(lower, color='red', linestyle='--', linewidth=2, label=f'Lower CI: {lower:.3f}')\n",
    "    plt.axvline(upper, color='red', linestyle='--', linewidth=2, label=f'Upper CI: {upper:.3f}')\n",
    "    plt.xlabel('Accuracy')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Bootstrap Distribution of Accuracy', fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    return mean_acc, lower, upper\n",
    "\n",
    "# Calculate confidence intervals\n",
    "mean_accuracy, ci_lower, ci_upper = bootstrap_confidence_intervals(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0921aa",
   "metadata": {},
   "source": [
    "## 🎯 Exercise: Advanced Evaluation Challenge\n",
    "\n",
    "Complete these advanced evaluation tasks:\n",
    "\n",
    "1. **Medical AI Metrics**: Implement sensitivity, specificity, PPV, NPV for each class\n",
    "2. **Cost-Sensitive Evaluation**: Define misclassification costs (e.g., missing cancer = high cost)\n",
    "3. **Threshold Optimization**: Find optimal decision thresholds for each class\n",
    "4. **Calibration Analysis**: Evaluate probability calibration using reliability diagrams\n",
    "\n",
    "### Expected Outcomes\n",
    "Your evaluation should demonstrate:\n",
    "- **Statistical significance**: p-value < 0.05\n",
    "- **Tight confidence intervals**: CI width < 0.1\n",
    "- **Balanced performance**: No class with <70% sensitivity\n",
    "- **Clinical relevance**: High NPV for cancer detection\n",
    "\n",
    "### Advanced Task\n",
    "Create a clinical decision support visualization showing prediction confidence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b86156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 VALIDATION: Advanced evaluation requirements\n",
    "def validate_evaluation_quality(eval_results, roc_scores, p_value, ci_width):\n",
    "    \"\"\"Validate that evaluation meets clinical standards\"\"\"\n",
    "    \n",
    "    print(\"🏥 Validating clinical evaluation standards...\")\n",
    "    \n",
    "    # Clinical performance requirements\n",
    "    balanced_acc = eval_results['balanced_accuracy']\n",
    "    kappa = eval_results['kappa']\n",
    "    \n",
    "    # ROC requirements\n",
    "    min_auc = min(roc_scores[i] for i in range(len(class_names)))\n",
    "    \n",
    "    print(f\"📊 Evaluation Quality Check:\")\n",
    "    print(f\"   Balanced Accuracy: {balanced_acc:.4f}\")\n",
    "    print(f\"   Cohen's Kappa: {kappa:.4f}\")\n",
    "    print(f\"   Minimum AUC: {min_auc:.4f}\")\n",
    "    print(f\"   Statistical p-value: {p_value:.6f}\")\n",
    "    print(f\"   CI Width: {ci_width:.4f}\")\n",
    "    \n",
    "    # Validation checks\n",
    "    assert balanced_acc > 0.70, f\"Balanced accuracy too low for clinical use: {balanced_acc:.4f}\"\n",
    "    assert kappa > 0.60, f\"Cohen's Kappa indicates poor agreement: {kappa:.4f}\"\n",
    "    assert min_auc > 0.75, f\"Minimum AUC too low: {min_auc:.4f}\"\n",
    "    assert p_value < 0.05, f\"Results not statistically significant: {p_value:.6f}\"\n",
    "    assert ci_width < 0.15, f\"Confidence interval too wide: {ci_width:.4f}\"\n",
    "    \n",
    "    print(\"\\n🎉 All clinical evaluation standards met!\")\n",
    "    print(\"🏥 Model ready for clinical validation studies!\")\n",
    "    print(\"🚀 Ready for next tutorial: Cross-validation & Advanced Techniques\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Run validation\n",
    "ci_width = ci_upper - ci_lower\n",
    "validate_evaluation_quality(eval_results, roc_scores, p_value, ci_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc12e297",
   "metadata": {},
   "source": [
    "## 📚 Summary\n",
    "\n",
    "You've mastered advanced model evaluation techniques:\n",
    "\n",
    "1. **Comprehensive Metrics**: Accuracy, balanced accuracy, Cohen's kappa, MCC\n",
    "2. **Medical AI Standards**: Sensitivity, specificity, PPV, NPV for each class\n",
    "3. **ROC & PR Analysis**: Multi-class curve analysis with AUC scores\n",
    "4. **Statistical Validation**: Permutation testing for significance\n",
    "5. **Confidence Intervals**: Bootstrap sampling for uncertainty quantification\n",
    "\n",
    "### Clinical Relevance\n",
    "- **Balanced Accuracy**: Addresses class imbalance common in medical data\n",
    "- **Cohen's Kappa**: Measures agreement beyond chance\n",
    "- **Statistical Testing**: Ensures results aren't due to random chance\n",
    "- **Confidence Intervals**: Quantifies uncertainty for clinical decisions\n",
    "\n",
    "### Best Practices\n",
    "✅ **Always use balanced metrics** for imbalanced medical datasets  \n",
    "✅ **Report confidence intervals** for all performance measures  \n",
    "✅ **Test statistical significance** before clinical deployment  \n",
    "✅ **Analyze per-class performance** especially for rare diseases  \n",
    "\n",
    "🎓 **Outstanding!** You're ready to evaluate medical AI systems professionally!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
