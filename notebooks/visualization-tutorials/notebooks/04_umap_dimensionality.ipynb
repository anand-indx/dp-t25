{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29904902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic Dataset Preparation for UMAP and Dimensionality Reduction (persistent across notebooks)\n",
    "# Automatic Dataset Preparation for UMAP and Dimensionality Reduction (persistent across notebooks)\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np, json\n",
    "\n",
    "# Bootstrap shared utils (Colab-friendly)\n",
    "try:\n",
    "    from shared import utils as u\n",
    "except ImportError:\n",
    "    repo_url = \"https://github.com/anand-indx/dp-t25.git\"; dest = \"/content/dp-t25\"\n",
    "    if 'google.colab' in sys.modules and not os.path.exists(dest):\n",
    "        import subprocess\n",
    "        subprocess.run(['git', 'clone', '--depth', '1', repo_url, dest], check=False)\n",
    "        sys.path.insert(0, dest)\n",
    "    else:\n",
    "        sys.path.insert(0, str(Path.cwd().parents[1]))\n",
    "    from shared import utils as u\n",
    "\n",
    "DATA_DIR = u.get_data_dir()\n",
    "UMAP_DIR = DATA_DIR / \"dimensionality_data\"\n",
    "UMAP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "from sklearn.datasets import make_classification, make_blobs\n",
    "\n",
    "def create_dimensionality_datasets():\n",
    "    \"\"\"Create high-dimensional datasets for UMAP and dimensionality reduction\"\"\"\n",
    "    np.random.seed(789)  # For reproducible embeddings\n",
    "\n",
    "    datasets = {}\n",
    "\n",
    "    # 1. High-dimensional pathology feature dataset\n",
    "    print(\"Creating high-dimensional pathology features...\")\n",
    "\n",
    "    n_samples = 800\n",
    "    n_features = 150\n",
    "    n_classes = 4\n",
    "\n",
    "    # Generate classification dataset with realistic pathology structure\n",
    "    X, y = make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=n_features,\n",
    "        n_informative=int(n_features * 0.7),  # 70% informative features\n",
    "        n_redundant=int(n_features * 0.2),    # 20% redundant features\n",
    "        n_clusters_per_class=2,\n",
    "        n_classes=n_classes,\n",
    "        class_sep=1.2,\n",
    "        flip_y=0.05,  # 5% label noise\n",
    "        random_state=789\n",
    "    )\n",
    "\n",
    "    # Create meaningful feature names\n",
    "    morphology_features = [f'morphology_{i:03d}' for i in range(50)]\n",
    "    texture_features = [f'texture_{i:03d}' for i in range(30)]\n",
    "    intensity_features = [f'intensity_{i:03d}' for i in range(25)]\n",
    "    shape_features = [f'shape_{i:03d}' for i in range(20)]\n",
    "    color_features = [f'color_{i:03d}' for i in range(15)]\n",
    "    spatial_features = [f'spatial_{i:03d}' for i in range(10)]\n",
    "\n",
    "    feature_names = (morphology_features + texture_features + intensity_features +\n",
    "                    shape_features + color_features + spatial_features)\n",
    "\n",
    "    # Create DataFrame\n",
    "    pathology_df = pd.DataFrame(X, columns=feature_names)\n",
    "    pathology_df['sample_id'] = [f'P{i:04d}' for i in range(n_samples)]\n",
    "\n",
    "    # Add meaningful class labels\n",
    "    class_labels = ['Normal_Tissue', 'Low_Grade_Tumor', 'High_Grade_Tumor', 'Metastatic']\n",
    "    pathology_df['tissue_class'] = [class_labels[label] for label in y]\n",
    "    pathology_df['numeric_class'] = y\n",
    "\n",
    "    # Add additional metadata\n",
    "    pathology_df['patient_age'] = np.random.normal(65, 12, n_samples).astype(int)\n",
    "    pathology_df['tumor_size'] = np.where(y > 0, np.random.lognormal(3, 0.5, n_samples), 0)\n",
    "    pathology_df['grade'] = np.where(y == 0, 0,\n",
    "                                   np.where(y == 1, 1,\n",
    "                                          np.where(y == 2, 2, 3)))\n",
    "\n",
    "    pathology_file = UMAP_DIR / \"high_dim_pathology_features.csv\"\n",
    "    pathology_df.to_csv(pathology_file, index=False)\n",
    "    datasets['pathology_features'] = str(pathology_file)\n",
    "\n",
    "    # 2. Multi-modal dataset (imaging + genomics + clinical)\n",
    "    print(\"Building multi-modal dataset...\")\n",
    "\n",
    "    n_multimodal = 500\n",
    "\n",
    "    # Imaging features (extracted from CNNs, etc.)\n",
    "    imaging_features = np.random.normal(0, 1, (n_multimodal, 50))\n",
    "\n",
    "    # Genomic features (mutation status, expression levels)\n",
    "    genomic_features = np.random.lognormal(0, 1, (n_multimodal, 100))\n",
    "\n",
    "    # Clinical features (normalized)\n",
    "    clinical_features = np.random.beta(2, 3, (n_multimodal, 20))\n",
    "\n",
    "    # Combine all modalities\n",
    "    multimodal_features = np.hstack([imaging_features, genomic_features, clinical_features])\n",
    "\n",
    "    # Create structured outcomes\n",
    "    outcome_probs = np.mean(multimodal_features[:, :30], axis=1)\n",
    "    outcomes = (outcome_probs > np.median(outcome_probs)).astype(int)\n",
    "\n",
    "    multimodal_df = pd.DataFrame(multimodal_features,\n",
    "                               columns=([f'imaging_{i:02d}' for i in range(50)] +\n",
    "                                       [f'genomic_{i:02d}' for i in range(100)] +\n",
    "                                       [f'clinical_{i:02d}' for i in range(20)]))\n",
    "\n",
    "    multimodal_df['patient_id'] = [f'M{i:04d}' for i in range(n_multimodal)]\n",
    "    multimodal_df['outcome'] = outcomes\n",
    "    multimodal_df['modality_imaging'] = 1\n",
    "    multimodal_df['modality_genomic'] = np.random.choice([0, 1], n_multimodal, p=[0.3, 0.7])\n",
    "    multimodal_df['modality_clinical'] = 1\n",
    "\n",
    "    multimodal_file = UMAP_DIR / \"multimodal_features.csv\"\n",
    "    multimodal_df.to_csv(multimodal_file, index=False)\n",
    "    datasets['multimodal_features'] = str(multimodal_file)\n",
    "\n",
    "    # 3. Clustering validation dataset\n",
    "    print(\"Creating clustering validation dataset...\")\n",
    "\n",
    "    cluster_centers = 8\n",
    "    cluster_std = 2.0\n",
    "\n",
    "    X_blobs, y_blobs = make_blobs(\n",
    "        n_samples=600,\n",
    "        centers=cluster_centers,\n",
    "        n_features=75,\n",
    "        cluster_std=cluster_std,\n",
    "        random_state=789,\n",
    "    )\n",
    "\n",
    "    cluster_types = ['Epithelial', 'Stromal', 'Immune_High', 'Immune_Low',\n",
    "                    'Necrotic', 'Vascular', 'Neural', 'Adipose']\n",
    "\n",
    "    clustering_df = pd.DataFrame(X_blobs, columns=[f'feature_{i:03d}' for i in range(75)])\n",
    "    clustering_df['sample_id'] = [f'C{i:04d}' for i in range(600)]\n",
    "    clustering_df['true_cluster'] = y_blobs\n",
    "    clustering_df['cluster_type'] = [cluster_types[cluster] for cluster in y_blobs]\n",
    "\n",
    "    noise_features = np.random.normal(0, 0.5, (600, 25))\n",
    "    noise_df = pd.DataFrame(noise_features, columns=[f'noise_{i:02d}' for i in range(25)])\n",
    "    clustering_df = pd.concat([clustering_df, noise_df], axis=1)\n",
    "\n",
    "    clustering_file = UMAP_DIR / \"clustering_validation.csv\"\n",
    "    clustering_df.to_csv(clustering_file, index=False)\n",
    "    datasets['clustering_validation'] = str(clustering_file)\n",
    "\n",
    "    # 4. Time-series embedding dataset\n",
    "    print(\"Generating time-series embedding data...\")\n",
    "\n",
    "    n_timepoints = 50\n",
    "    n_patients_ts = 100\n",
    "    n_biomarkers = 30\n",
    "\n",
    "    timeseries_embedding_data = []\n",
    "\n",
    "    for patient in range(n_patients_ts):\n",
    "        trajectory_type = np.random.choice(['stable', 'progressive', 'responsive', 'resistant'])\n",
    "        for t in range(n_timepoints):\n",
    "            biomarker_vector = []\n",
    "            for biomarker in range(n_biomarkers):\n",
    "                if trajectory_type == 'stable':\n",
    "                    value = 5 + np.random.normal(0, 0.5)\n",
    "                elif trajectory_type == 'progressive':\n",
    "                    value = 5 + 0.1 * t + np.random.normal(0, 0.8)\n",
    "                elif trajectory_type == 'responsive':\n",
    "                    value = 5 + 3 * np.exp(-0.1 * t) + np.random.normal(0, 0.6)\n",
    "                else:  # resistant\n",
    "                    value = 5 + 2 * (1 - np.exp(-0.05 * t)) + np.random.normal(0, 0.7)\n",
    "                biomarker_vector.append(value)\n",
    "            timeseries_embedding_data.append({\n",
    "                'patient_id': f'T{patient:03d}',\n",
    "                'timepoint': t,\n",
    "                'trajectory_type': trajectory_type,\n",
    "                **{f'biomarker_{i:02d}': val for i, val in enumerate(biomarker_vector)}\n",
    "            })\n",
    "\n",
    "    timeseries_embedding_df = pd.DataFrame(timeseries_embedding_data)\n",
    "    timeseries_embedding_file = UMAP_DIR / \"timeseries_embedding.csv\"\n",
    "    timeseries_embedding_df.to_csv(timeseries_embedding_file, index=False)\n",
    "    datasets['timeseries_embedding'] = str(timeseries_embedding_file)\n",
    "\n",
    "    metadata = {\n",
    "        'creation_info': {\n",
    "            'date': pd.Timestamp.now().isoformat(),\n",
    "            'purpose': 'UMAP and dimensionality reduction demonstrations',\n",
    "            'random_seed': 789,\n",
    "            'sklearn_version': 'Compatible with scikit-learn API'\n",
    "        },\n",
    "        'datasets': {\n",
    "            'pathology_features': {\n",
    "                'description': 'High-dimensional pathology features for tissue classification',\n",
    "                'dimensions': f\"{n_samples} samples × {n_features} features\",\n",
    "                'classes': class_labels,\n",
    "            },\n",
    "            'multimodal_features': {\n",
    "                'description': 'Multi-modal dataset combining imaging, genomics, and clinical data',\n",
    "                'dimensions': f\"{n_multimodal} samples × {multimodal_features.shape[1]} features\",\n",
    "            },\n",
    "            'clustering_validation': {\n",
    "                'description': 'Blob clusters for clustering algorithm validation',\n",
    "                'n_clusters': cluster_centers,\n",
    "                'cluster_types': cluster_types,\n",
    "                'dimensions': \"600 samples × 100 features (75 informative + 25 noise)\",\n",
    "            },\n",
    "            'timeseries_embedding': {\n",
    "                'description': 'Time-series biomarker data for trajectory analysis',\n",
    "                'n_patients': n_patients_ts,\n",
    "                'n_timepoints': n_timepoints,\n",
    "                'n_biomarkers': n_biomarkers,\n",
    "                'trajectory_types': ['stable', 'progressive', 'responsive', 'resistant']\n",
    "            },\n",
    "        },\n",
    "        'umap_parameters': {\n",
    "            'recommended_settings': {\n",
    "                'n_neighbors': [5, 15, 50],\n",
    "                'min_dist': [0.1, 0.3, 0.5],\n",
    "                'n_components': [2, 3],\n",
    "                'metric': ['euclidean', 'manhattan', 'cosine']\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    metadata_file = UMAP_DIR / \"dimensionality_metadata.json\"\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "\n",
    "    print(f\"Dimensionality reduction datasets ready! {len(datasets)} datasets created\")\n",
    "    print(f\"Data location: {UMAP_DIR}\")\n",
    "\n",
    "    return UMAP_DIR, datasets, metadata\n",
    "\n",
    "# Generate datasets\n",
    "umap_data_dir, available_umap_datasets, umap_metadata = create_dimensionality_datasets()\n",
    "\n",
    "print(\"Available dimensionality reduction datasets:\")\n",
    "for name, path in available_umap_datasets.items():\n",
    "    print(f\"  - {name}: {Path(path).name}\")\n",
    "\n",
    "# Optional: UMAP availability notice\n",
    "try:\n",
    "    import umap\n",
    "    print(\"UMAP is available for use\")\n",
    "except ImportError:\n",
    "    print(\"UMAP not installed. Run: pip install umap-learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea5020f",
   "metadata": {},
   "source": [
    "# UMAP and Dimensionality Reduction for Digital Pathology\n",
    "\n",
    "This notebook covers advanced dimensionality reduction techniques including UMAP, t-SNE, and PCA specifically for digital pathology data analysis. Learn to visualize high-dimensional pathology features and discover hidden patterns.\n",
    "\n",
    "## Learning Objectives\n",
    "1. Master UMAP for non-linear dimensionality reduction\n",
    "2. Compare t-SNE, PCA, and UMAP techniques\n",
    "3. Optimize hyperparameters for pathology data\n",
    "4. Create interactive dimensionality reduction plots\n",
    "5. Analyze clusters and subpopulations in pathology data\n",
    "6. Validate dimensionality reduction results\n",
    "\n",
    "## Prerequisites\n",
    "- Completed correlation and statistical visualization notebooks\n",
    "- Understanding of high-dimensional data concepts\n",
    "- Basic knowledge of machine learning principles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb557c32",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bfc210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "import umap.umap_ as umap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Try to import plotly for interactive plots (optional)\n",
    "try:\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    PLOTLY_AVAILABLE = True\n",
    "    print(\"✅ Plotly available for interactive visualizations\")\n",
    "except ImportError:\n",
    "    PLOTLY_AVAILABLE = False\n",
    "    print(\"⚠️ Plotly not available - using matplotlib for static plots\")\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4151adf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive high-dimensional pathology dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 800  # More samples for better dimensionality reduction\n",
    "\n",
    "print(\"Creating comprehensive pathology dataset...\")\n",
    "\n",
    "# Define different tissue/cancer types for ground truth clusters\n",
    "tissue_types = ['Normal', 'Benign', 'Low_Grade_Cancer', 'High_Grade_Cancer']\n",
    "tissue_probs = [0.25, 0.25, 0.25, 0.25]\n",
    "tissue_labels = np.random.choice(tissue_types, n_samples, p=tissue_probs)\n",
    "\n",
    "# Nuclear morphology features (50 features simulating different nuclear measurements)\n",
    "print(\"• Nuclear morphology features...\")\n",
    "nuclear_features = {}\n",
    "for i in range(1, 21):  # 20 nuclear features\n",
    "    base_values = {\n",
    "        'Normal': np.random.normal(100, 15, n_samples),\n",
    "        'Benign': np.random.normal(110, 18, n_samples), \n",
    "        'Low_Grade_Cancer': np.random.normal(130, 25, n_samples),\n",
    "        'High_Grade_Cancer': np.random.normal(160, 35, n_samples)\n",
    "    }\n",
    "    \n",
    "    nuclear_features[f'nuclear_feature_{i:02d}'] = np.concatenate([\n",
    "        base_values[tissue][tissue_labels == tissue] for tissue in tissue_types\n",
    "    ])\n",
    "\n",
    "# Chromatin texture features (30 features)\n",
    "print(\"• Chromatin texture features...\")\n",
    "chromatin_features = {}\n",
    "for i in range(1, 16):  # 15 chromatin features\n",
    "    base_values = {\n",
    "        'Normal': np.random.normal(0.5, 0.1, n_samples),\n",
    "        'Benign': np.random.normal(0.6, 0.12, n_samples),\n",
    "        'Low_Grade_Cancer': np.random.normal(0.75, 0.15, n_samples),\n",
    "        'High_Grade_Cancer': np.random.normal(0.9, 0.2, n_samples)\n",
    "    }\n",
    "    \n",
    "    chromatin_features[f'chromatin_feature_{i:02d}'] = np.concatenate([\n",
    "        base_values[tissue][tissue_labels == tissue] for tissue in tissue_types\n",
    "    ])\n",
    "\n",
    "# Cellular organization features (25 features)\n",
    "print(\"• Cellular organization features...\")\n",
    "organization_features = {}\n",
    "for i in range(1, 11):  # 10 organization features\n",
    "    base_values = {\n",
    "        'Normal': np.random.normal(1.0, 0.2, n_samples),\n",
    "        'Benign': np.random.normal(0.8, 0.25, n_samples),\n",
    "        'Low_Grade_Cancer': np.random.normal(0.6, 0.3, n_samples),\n",
    "        'High_Grade_Cancer': np.random.normal(0.3, 0.35, n_samples)\n",
    "    }\n",
    "    \n",
    "    organization_features[f'organization_feature_{i:02d}'] = np.concatenate([\n",
    "        base_values[tissue][tissue_labels == tissue] for tissue in tissue_types\n",
    "    ])\n",
    "\n",
    "# Vascular and stromal features (15 features)\n",
    "print(\"• Vascular and stromal features...\")\n",
    "vascular_features = {}\n",
    "for i in range(1, 8):  # 7 vascular features\n",
    "    base_values = {\n",
    "        'Normal': np.random.normal(20, 5, n_samples),\n",
    "        'Benign': np.random.normal(25, 6, n_samples),\n",
    "        'Low_Grade_Cancer': np.random.normal(35, 8, n_samples),\n",
    "        'High_Grade_Cancer': np.random.normal(50, 12, n_samples)\n",
    "    }\n",
    "    \n",
    "    vascular_features[f'vascular_feature_{i:02d}'] = np.concatenate([\n",
    "        base_values[tissue][tissue_labels == tissue] for tissue in tissue_types\n",
    "    ])\n",
    "\n",
    "# Immune infiltration features (20 features)\n",
    "print(\"• Immune infiltration features...\")\n",
    "immune_features = {}\n",
    "for i in range(1, 11):  # 10 immune features\n",
    "    base_values = {\n",
    "        'Normal': np.random.normal(30, 8, n_samples),\n",
    "        'Benign': np.random.normal(45, 10, n_samples),\n",
    "        'Low_Grade_Cancer': np.random.normal(60, 15, n_samples),\n",
    "        'High_Grade_Cancer': np.random.normal(80, 20, n_samples)\n",
    "    }\n",
    "    \n",
    "    immune_features[f'immune_feature_{i:02d}'] = np.concatenate([\n",
    "        base_values[tissue][tissue_labels == tissue] for tissue in tissue_types\n",
    "    ])\n",
    "\n",
    "# Combine all features\n",
    "all_features = {**nuclear_features, **chromatin_features, **organization_features, \n",
    "               **vascular_features, **immune_features}\n",
    "\n",
    "# Create DataFrame\n",
    "pathology_data = pd.DataFrame(all_features)\n",
    "pathology_data['tissue_type'] = tissue_labels\n",
    "pathology_data['patient_id'] = [f'P{i:04d}' for i in range(1, n_samples + 1)]\n",
    "\n",
    "# Add some noise and correlations to make it more realistic\n",
    "for i in range(0, len(pathology_data.columns) - 2, 3):  # Every 3rd feature correlates\n",
    "    if i + 1 < len(pathology_data.columns) - 2:\n",
    "        col1 = pathology_data.columns[i]\n",
    "        col2 = pathology_data.columns[i + 1]\n",
    "        pathology_data[col2] = pathology_data[col1] * 0.7 + pathology_data[col2] * 0.3\n",
    "\n",
    "print(f\"✅ High-dimensional pathology dataset created:\")\n",
    "print(f\"   • {len(pathology_data)} samples\")\n",
    "print(f\"   • {len(pathology_data.columns) - 2} features\")\n",
    "print(f\"   • 4 tissue types: {tissue_types}\")\n",
    "print(f\"   • Feature categories: Nuclear (20), Chromatin (15), Organization (10), Vascular (7), Immune (10)\")\n",
    "\n",
    "# Display class distribution\n",
    "print(f\"\\nClass distribution:\")\n",
    "for tissue, count in pd.Series(tissue_labels).value_counts().items():\n",
    "    print(f\"   • {tissue}: {count} samples ({count/len(tissue_labels)*100:.1f}%)\")\n",
    "\n",
    "pathology_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f62994",
   "metadata": {},
   "source": [
    "## 2. Principal Component Analysis (PCA)\n",
    "\n",
    "Start with linear dimensionality reduction using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a600e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for PCA\n",
    "feature_columns = [col for col in pathology_data.columns if col not in ['tissue_type', 'patient_id']]\n",
    "X = pathology_data[feature_columns].values\n",
    "y = pathology_data['tissue_type'].values\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"Data prepared for PCA: {X_scaled.shape}\")\n",
    "\n",
    "# Perform PCA\n",
    "print(\"Performing PCA analysis...\")\n",
    "pca_full = PCA()\n",
    "X_pca_full = pca_full.fit_transform(X_scaled)\n",
    "\n",
    "# Calculate explained variance\n",
    "explained_variance_ratio = pca_full.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Find optimal number of components for 95% variance\n",
    "n_components_95 = np.where(cumulative_variance >= 0.95)[0][0] + 1\n",
    "n_components_90 = np.where(cumulative_variance >= 0.90)[0][0] + 1\n",
    "\n",
    "print(f\"Components for 90% variance: {n_components_90}\")\n",
    "print(f\"Components for 95% variance: {n_components_95}\")\n",
    "\n",
    "# Create PCA visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Scree plot\n",
    "axes[0, 0].plot(range(1, min(21, len(explained_variance_ratio) + 1)), \n",
    "                explained_variance_ratio[:20], 'bo-', markersize=6)\n",
    "axes[0, 0].set_xlabel('Principal Component')\n",
    "axes[0, 0].set_ylabel('Explained Variance Ratio')\n",
    "axes[0, 0].set_title('PCA Scree Plot (First 20 Components)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add elbow point annotation\n",
    "if len(explained_variance_ratio) > 2:\n",
    "    elbow_point = np.where(explained_variance_ratio > 0.05)[0][-1] + 1 if any(explained_variance_ratio > 0.05) else 3\n",
    "    axes[0, 0].axvline(x=elbow_point, color='red', linestyle='--', alpha=0.7, label=f'Elbow at PC{elbow_point}')\n",
    "    axes[0, 0].legend()\n",
    "\n",
    "# 2. Cumulative variance explained\n",
    "axes[0, 1].plot(range(1, min(31, len(cumulative_variance) + 1)), \n",
    "                cumulative_variance[:30], 'ro-', markersize=4)\n",
    "axes[0, 1].axhline(y=0.90, color='orange', linestyle='--', alpha=0.7, label='90% variance')\n",
    "axes[0, 1].axhline(y=0.95, color='green', linestyle='--', alpha=0.7, label='95% variance')\n",
    "axes[0, 1].set_xlabel('Number of Components')\n",
    "axes[0, 1].set_ylabel('Cumulative Explained Variance')\n",
    "axes[0, 1].set_title('Cumulative Variance Explained')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. PCA 2D projection\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_pca_2d = pca_2d.fit_transform(X_scaled)\n",
    "\n",
    "colors = {'Normal': 'blue', 'Benign': 'green', 'Low_Grade_Cancer': 'orange', 'High_Grade_Cancer': 'red'}\n",
    "for tissue in tissue_types:\n",
    "    mask = y == tissue\n",
    "    axes[1, 0].scatter(X_pca_2d[mask, 0], X_pca_2d[mask, 1], \n",
    "                      c=colors[tissue], label=tissue, alpha=0.7, s=30)\n",
    "\n",
    "axes[1, 0].set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.1%} variance)')\n",
    "axes[1, 0].set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.1%} variance)')\n",
    "axes[1, 0].set_title('PCA 2D Projection')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Feature loadings for PC1 and PC2\n",
    "feature_loadings = pca_2d.components_.T\n",
    "feature_importance = np.sqrt(feature_loadings[:, 0]**2 + feature_loadings[:, 1]**2)\n",
    "\n",
    "# Show top 10 most important features\n",
    "top_features_idx = np.argsort(feature_importance)[-10:]\n",
    "top_features = [feature_columns[i] for i in top_features_idx]\n",
    "top_loadings = feature_importance[top_features_idx]\n",
    "\n",
    "axes[1, 1].barh(range(len(top_features)), top_loadings, color='skyblue')\n",
    "axes[1, 1].set_yticks(range(len(top_features)))\n",
    "axes[1, 1].set_yticklabels([f.replace('_', ' ')[:15] + '...' if len(f) > 15 else f.replace('_', ' ') \n",
    "                           for f in top_features])\n",
    "axes[1, 1].set_xlabel('Loading Magnitude')\n",
    "axes[1, 1].set_title('Top 10 Feature Loadings (PC1 + PC2)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print PCA summary\n",
    "print(\"=== PCA ANALYSIS SUMMARY ===\")\n",
    "print(f\"Total features: {X_scaled.shape[1]}\")\n",
    "print(f\"First 2 components explain: {pca_2d.explained_variance_ratio_.sum():.1%} of variance\")\n",
    "print(f\"First 5 components explain: {explained_variance_ratio[:5].sum():.1%} of variance\")\n",
    "print(f\"Components needed for 95% variance: {n_components_95}\")\n",
    "\n",
    "print(f\"\\nTop contributing features to PC1 & PC2:\")\n",
    "for i, (feature, importance) in enumerate(zip(top_features[-5:], top_loadings[-5:]), 1):\n",
    "    print(f\"{i}. {feature}: {importance:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8649ca9",
   "metadata": {},
   "source": [
    "## 3. t-SNE Analysis\n",
    "\n",
    "Apply t-SNE for non-linear dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6150ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE analysis with different perplexity values\n",
    "print(\"Performing t-SNE analysis...\")\n",
    "\n",
    "# Test different perplexity values\n",
    "perplexity_values = [5, 15, 30, 50]\n",
    "tsne_results = {}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, perplexity in enumerate(perplexity_values):\n",
    "    print(f\"Computing t-SNE with perplexity = {perplexity}...\")\n",
    "    \n",
    "    # Limit features for faster computation\n",
    "    X_sample = X_scaled[:, :30]  # Use first 30 features for speed\n",
    "    \n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42, \n",
    "               n_iter=1000, learning_rate=200)\n",
    "    X_tsne = tsne.fit_transform(X_sample)\n",
    "    \n",
    "    tsne_results[perplexity] = X_tsne\n",
    "    \n",
    "    # Plot results\n",
    "    for tissue in tissue_types:\n",
    "        mask = y == tissue\n",
    "        axes[i].scatter(X_tsne[mask, 0], X_tsne[mask, 1], \n",
    "                       c=colors[tissue], label=tissue, alpha=0.7, s=20)\n",
    "    \n",
    "    axes[i].set_title(f't-SNE (perplexity = {perplexity})')\n",
    "    axes[i].set_xlabel('t-SNE 1')\n",
    "    axes[i].set_ylabel('t-SNE 2')\n",
    "    if i == 0:\n",
    "        axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze t-SNE clustering quality\n",
    "print(\"\\n=== t-SNE CLUSTERING QUALITY ===\")\n",
    "\n",
    "for perplexity, X_tsne in tsne_results.items():\n",
    "    # Calculate silhouette score\n",
    "    le = LabelEncoder()\n",
    "    y_numeric = le.fit_transform(y)\n",
    "    sil_score = silhouette_score(X_tsne, y_numeric)\n",
    "    \n",
    "    print(f\"Perplexity {perplexity:2d}: Silhouette Score = {sil_score:.3f}\")\n",
    "\n",
    "# Find best perplexity\n",
    "best_perplexity = max(tsne_results.keys(), \n",
    "                     key=lambda p: silhouette_score(tsne_results[p], le.fit_transform(y)))\n",
    "print(f\"\\nBest perplexity: {best_perplexity} (highest silhouette score)\")\n",
    "\n",
    "# Detailed analysis of best t-SNE result\n",
    "X_tsne_best = tsne_results[best_perplexity]\n",
    "\n",
    "# Calculate cluster separation metrics\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "print(f\"\\n=== DETAILED t-SNE ANALYSIS (Perplexity = {best_perplexity}) ===\")\n",
    "\n",
    "# Calculate within-class and between-class distances\n",
    "within_class_distances = []\n",
    "between_class_distances = []\n",
    "\n",
    "for tissue in tissue_types:\n",
    "    mask = y == tissue\n",
    "    tissue_points = X_tsne_best[mask]\n",
    "    \n",
    "    if len(tissue_points) > 1:\n",
    "        # Within-class distances\n",
    "        within_dist = pdist(tissue_points)\n",
    "        within_class_distances.extend(within_dist)\n",
    "        \n",
    "        # Between-class distances to other tissues\n",
    "        for other_tissue in tissue_types:\n",
    "            if other_tissue != tissue:\n",
    "                other_mask = y == other_tissue\n",
    "                other_points = X_tsne_best[other_mask]\n",
    "                \n",
    "                for point1 in tissue_points:\n",
    "                    for point2 in other_points:\n",
    "                        between_dist = np.linalg.norm(point1 - point2)\n",
    "                        between_class_distances.append(between_dist)\n",
    "\n",
    "avg_within = np.mean(within_class_distances)\n",
    "avg_between = np.mean(between_class_distances)\n",
    "separation_ratio = avg_between / avg_within\n",
    "\n",
    "print(f\"Average within-class distance: {avg_within:.2f}\")\n",
    "print(f\"Average between-class distance: {avg_between:.2f}\")\n",
    "print(f\"Separation ratio: {separation_ratio:.2f}\")\n",
    "\n",
    "if separation_ratio > 2:\n",
    "    print(\"✅ Excellent class separation!\")\n",
    "elif separation_ratio > 1.5:\n",
    "    print(\"✅ Good class separation\")\n",
    "elif separation_ratio > 1.2:\n",
    "    print(\"⚠️ Moderate class separation\")\n",
    "else:\n",
    "    print(\"❌ Poor class separation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a4bfbe",
   "metadata": {},
   "source": [
    "## 4. UMAP Analysis\n",
    "\n",
    "Apply UMAP for state-of-the-art non-linear dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f5e866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP analysis with hyperparameter optimization\n",
    "print(\"Performing UMAP analysis...\")\n",
    "\n",
    "# Define hyperparameter grid\n",
    "n_neighbors_values = [5, 15, 30, 50]\n",
    "min_dist_values = [0.01, 0.1, 0.3, 0.5]\n",
    "\n",
    "# Test key hyperparameter combinations\n",
    "hyperparameter_combinations = [\n",
    "    (15, 0.1),  # Default-like\n",
    "    (5, 0.01),   # Local structure focus\n",
    "    (50, 0.3),   # Global structure focus  \n",
    "    (30, 0.1)    # Balanced\n",
    "]\n",
    "\n",
    "umap_results = {}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (n_neighbors, min_dist) in enumerate(hyperparameter_combinations):\n",
    "    print(f\"Computing UMAP with n_neighbors = {n_neighbors}, min_dist = {min_dist}...\")\n",
    "    \n",
    "    umap_reducer = umap.UMAP(n_neighbors=n_neighbors, \n",
    "                            min_dist=min_dist,\n",
    "                            n_components=2, \n",
    "                            random_state=42,\n",
    "                            metric='euclidean')\n",
    "    \n",
    "    X_umap = umap_reducer.fit_transform(X_scaled)\n",
    "    umap_results[(n_neighbors, min_dist)] = X_umap\n",
    "    \n",
    "    # Plot results\n",
    "    for tissue in tissue_types:\n",
    "        mask = y == tissue\n",
    "        axes[i].scatter(X_umap[mask, 0], X_umap[mask, 1], \n",
    "                       c=colors[tissue], label=tissue, alpha=0.7, s=20)\n",
    "    \n",
    "    axes[i].set_title(f'UMAP (neighbors={n_neighbors}, min_dist={min_dist})')\n",
    "    axes[i].set_xlabel('UMAP 1')\n",
    "    axes[i].set_ylabel('UMAP 2')\n",
    "    if i == 0:\n",
    "        axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate UMAP results\n",
    "print(\"\\n=== UMAP HYPERPARAMETER EVALUATION ===\")\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_numeric = le.fit_transform(y)\n",
    "\n",
    "umap_evaluation = []\n",
    "\n",
    "for (n_neighbors, min_dist), X_umap in umap_results.items():\n",
    "    # Silhouette score\n",
    "    sil_score = silhouette_score(X_umap, y_numeric)\n",
    "    \n",
    "    # Custom cluster separation metric\n",
    "    within_distances = []\n",
    "    between_distances = []\n",
    "    \n",
    "    for tissue_idx, tissue in enumerate(tissue_types):\n",
    "        mask = y == tissue\n",
    "        tissue_points = X_umap[mask]\n",
    "        \n",
    "        if len(tissue_points) > 1:\n",
    "            # Within-tissue distances\n",
    "            within_dist = pdist(tissue_points)\n",
    "            within_distances.extend(within_dist)\n",
    "            \n",
    "            # Between-tissue distances\n",
    "            for other_tissue_idx, other_tissue in enumerate(tissue_types):\n",
    "                if other_tissue_idx > tissue_idx:  # Avoid duplicates\n",
    "                    other_mask = y == other_tissue\n",
    "                    other_points = X_umap[other_mask]\n",
    "                    \n",
    "                    # Sample distances to avoid computation explosion\n",
    "                    sample_size = min(50, len(tissue_points), len(other_points))\n",
    "                    tissue_sample = tissue_points[np.random.choice(len(tissue_points), sample_size, replace=False)]\n",
    "                    other_sample = other_points[np.random.choice(len(other_points), sample_size, replace=False)]\n",
    "                    \n",
    "                    for point1 in tissue_sample:\n",
    "                        for point2 in other_sample:\n",
    "                            between_distances.append(np.linalg.norm(point1 - point2))\n",
    "    \n",
    "    avg_within = np.mean(within_distances) if within_distances else 0\n",
    "    avg_between = np.mean(between_distances) if between_distances else 0\n",
    "    separation_ratio = avg_between / avg_within if avg_within > 0 else 0\n",
    "    \n",
    "    umap_evaluation.append({\n",
    "        'n_neighbors': n_neighbors,\n",
    "        'min_dist': min_dist,\n",
    "        'silhouette_score': sil_score,\n",
    "        'separation_ratio': separation_ratio,\n",
    "        'avg_within_distance': avg_within,\n",
    "        'avg_between_distance': avg_between\n",
    "    })\n",
    "    \n",
    "    print(f\"n_neighbors={n_neighbors:2d}, min_dist={min_dist:.2f}: \"\n",
    "          f\"Silhouette={sil_score:.3f}, Separation={separation_ratio:.2f}\")\n",
    "\n",
    "# Find best UMAP configuration\n",
    "eval_df = pd.DataFrame(umap_evaluation)\n",
    "best_config_idx = eval_df['silhouette_score'].idxmax()\n",
    "best_config = eval_df.iloc[best_config_idx]\n",
    "\n",
    "print(f\"\\n✅ Best UMAP configuration:\")\n",
    "print(f\"   n_neighbors = {best_config['n_neighbors']}\")\n",
    "print(f\"   min_dist = {best_config['min_dist']}\")\n",
    "print(f\"   Silhouette score = {best_config['silhouette_score']:.3f}\")\n",
    "print(f\"   Separation ratio = {best_config['separation_ratio']:.2f}\")\n",
    "\n",
    "# Store best UMAP result\n",
    "best_umap_key = (int(best_config['n_neighbors']), best_config['min_dist'])\n",
    "X_umap_best = umap_results[best_umap_key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bb2ee7",
   "metadata": {},
   "source": [
    "## 5. Method Comparison\n",
    "\n",
    "Compare PCA, t-SNE, and UMAP side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a0b345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure DATA_DIR exists (from setup). Not directly used here but kept for consistency.\n",
    "try:\n",
    "    DATA_DIR\n",
    "except NameError:\n",
    "    from shared import utils as u\n",
    "    DATA_DIR = u.get_data_dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b894f34",
   "metadata": {},
   "source": [
    "## 6. Cluster Analysis and Discovery\n",
    "\n",
    "Analyze clusters discovered by dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9863510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster analysis on embeddings\n",
    "print(\"=== CLUSTER DISCOVERY ANALYSIS ===\")\n",
    "\n",
    "# Use best embedding method for cluster analysis\n",
    "best_embedding_method = metrics_df.loc[metrics_df['Silhouette_Score'].idxmax(), 'Method']\n",
    "best_embedding = methods_data[best_embedding_method]\n",
    "\n",
    "print(f\"Using {best_embedding_method} embedding for cluster analysis\")\n",
    "\n",
    "# Try different clustering algorithms\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "clustering_methods = {\n",
    "    'K-Means': KMeans(n_clusters=4, random_state=42),\n",
    "    'DBSCAN': DBSCAN(eps=1.5, min_samples=5),\n",
    "    'Hierarchical': AgglomerativeClustering(n_clusters=4),\n",
    "    'Gaussian Mixture': GaussianMixture(n_components=4, random_state=42)\n",
    "}\n",
    "\n",
    "cluster_results = {}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (method_name, clusterer) in enumerate(clustering_methods.items()):\n",
    "    print(f\"\\nApplying {method_name}...\")\n",
    "    \n",
    "    if method_name == 'Gaussian Mixture':\n",
    "        cluster_labels = clusterer.fit_predict(best_embedding)\n",
    "    else:\n",
    "        cluster_labels = clusterer.fit_predict(best_embedding)\n",
    "    \n",
    "    cluster_results[method_name] = cluster_labels\n",
    "    \n",
    "    # Handle noise points in DBSCAN (labeled as -1)\n",
    "    unique_labels = np.unique(cluster_labels)\n",
    "    n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)\n",
    "    \n",
    "    print(f\"Found {n_clusters} clusters\")\n",
    "    \n",
    "    # Plot results\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Color map for clusters\n",
    "    if -1 in unique_labels:  # DBSCAN with noise\n",
    "        colors_cluster = plt.cm.Set1(np.linspace(0, 1, len(unique_labels)))\n",
    "        colors_cluster = colors_cluster[unique_labels != -1]  # Remove noise color\n",
    "        \n",
    "        # Plot noise points\n",
    "        noise_mask = cluster_labels == -1\n",
    "        ax.scatter(best_embedding[noise_mask, 0], best_embedding[noise_mask, 1], \n",
    "                  c='black', marker='x', s=20, alpha=0.5, label='Noise')\n",
    "        \n",
    "        # Plot clusters\n",
    "        for j, label in enumerate(unique_labels):\n",
    "            if label != -1:\n",
    "                mask = cluster_labels == label\n",
    "                ax.scatter(best_embedding[mask, 0], best_embedding[mask, 1], \n",
    "                          c=[colors_cluster[j]], label=f'Cluster {label}', s=30, alpha=0.7)\n",
    "    else:\n",
    "        colors_cluster = plt.cm.Set1(np.linspace(0, 1, len(unique_labels)))\n",
    "        for j, label in enumerate(unique_labels):\n",
    "            mask = cluster_labels == label\n",
    "            ax.scatter(best_embedding[mask, 0], best_embedding[mask, 1], \n",
    "                      c=[colors_cluster[j]], label=f'Cluster {label}', s=30, alpha=0.7)\n",
    "    \n",
    "    ax.set_title(f'{method_name} Clustering')\n",
    "    ax.set_xlabel(f'{best_embedding_method} 1')\n",
    "    ax.set_ylabel(f'{best_embedding_method} 2')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Calculate clustering metrics\n",
    "    if n_clusters > 1:\n",
    "        # Silhouette score\n",
    "        valid_mask = cluster_labels != -1  # Exclude noise for DBSCAN\n",
    "        if np.sum(valid_mask) > 1:\n",
    "            sil_score = silhouette_score(best_embedding[valid_mask], cluster_labels[valid_mask])\n",
    "            \n",
    "            # Adjusted Rand Index with true labels\n",
    "            ari_score = adjusted_rand_score(y_numeric[valid_mask], cluster_labels[valid_mask])\n",
    "            \n",
    "            print(f\"  Silhouette Score: {sil_score:.3f}\")\n",
    "            print(f\"  Adjusted Rand Index: {ari_score:.3f}\")\n",
    "        else:\n",
    "            print(\"  Unable to calculate metrics (insufficient valid points)\")\n",
    "    else:\n",
    "        print(\"  Unable to calculate metrics (insufficient clusters)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed analysis of best clustering method\n",
    "best_clustering_method = None\n",
    "best_ari = -1\n",
    "\n",
    "for method_name, cluster_labels in cluster_results.items():\n",
    "    valid_mask = cluster_labels != -1\n",
    "    if np.sum(valid_mask) > 1 and len(np.unique(cluster_labels[valid_mask])) > 1:\n",
    "        ari_score = adjusted_rand_score(y_numeric[valid_mask], cluster_labels[valid_mask])\n",
    "        if ari_score > best_ari:\n",
    "            best_ari = ari_score\n",
    "            best_clustering_method = method_name\n",
    "\n",
    "if best_clustering_method:\n",
    "    print(f\"\\n🏆 Best clustering method: {best_clustering_method} (ARI = {best_ari:.3f})\")\n",
    "    \n",
    "    # Analyze agreement between discovered clusters and true tissue types\n",
    "    best_clusters = cluster_results[best_clustering_method]\n",
    "    \n",
    "    print(f\"\\n=== CLUSTER-TISSUE TYPE CORRESPONDENCE ===\")\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "    valid_mask = best_clusters != -1\n",
    "    if np.sum(valid_mask) > 0:\n",
    "        cm = confusion_matrix(y_numeric[valid_mask], best_clusters[valid_mask])\n",
    "        \n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "        \n",
    "        # Get unique cluster labels for labeling\n",
    "        unique_clusters = np.unique(best_clusters[valid_mask])\n",
    "        \n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   xticklabels=[f'Cluster {i}' for i in unique_clusters],\n",
    "                   yticklabels=tissue_types, ax=ax)\n",
    "        ax.set_title(f'Confusion Matrix: True Tissue Types vs {best_clustering_method} Clusters')\n",
    "        ax.set_ylabel('True Tissue Type')\n",
    "        ax.set_xlabel('Discovered Cluster')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Calculate purity scores\n",
    "        cluster_purities = []\n",
    "        for cluster_id in unique_clusters:\n",
    "            cluster_mask = (best_clusters == cluster_id) & valid_mask\n",
    "            if np.sum(cluster_mask) > 0:\n",
    "                cluster_tissue_counts = pd.Series(y[cluster_mask]).value_counts()\n",
    "                purity = cluster_tissue_counts.max() / cluster_tissue_counts.sum()\n",
    "                dominant_tissue = cluster_tissue_counts.idxmax()\n",
    "                cluster_purities.append({\n",
    "                    'Cluster': cluster_id,\n",
    "                    'Dominant_Tissue': dominant_tissue,\n",
    "                    'Purity': purity,\n",
    "                    'Size': np.sum(cluster_mask)\n",
    "                })\n",
    "        \n",
    "        purity_df = pd.DataFrame(cluster_purities)\n",
    "        print(f\"\\nCluster Purity Analysis:\")\n",
    "        for _, row in purity_df.iterrows():\n",
    "            print(f\"  Cluster {int(row['Cluster'])}: {row['Dominant_Tissue']} \"\n",
    "                  f\"({row['Purity']:.1%} purity, {int(row['Size'])} samples)\")\n",
    "        \n",
    "        average_purity = purity_df['Purity'].mean()\n",
    "        print(f\"\\nOverall average purity: {average_purity:.1%}\")\n",
    "        \n",
    "        if average_purity > 0.8:\n",
    "            print(\"✅ Excellent cluster-tissue correspondence!\")\n",
    "        elif average_purity > 0.6:\n",
    "            print(\"✅ Good cluster-tissue correspondence\")\n",
    "        elif average_purity > 0.4:\n",
    "            print(\"⚠️ Moderate cluster-tissue correspondence\")\n",
    "        else:\n",
    "            print(\"❌ Poor cluster-tissue correspondence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d4ff87",
   "metadata": {},
   "source": [
    "## 7. Interactive Visualization (if Plotly available)\n",
    "\n",
    "Create interactive plots for better exploration of the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05b51b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive visualization\n",
    "if PLOTLY_AVAILABLE:\n",
    "    print(\"Creating interactive visualizations with Plotly...\")\n",
    "    \n",
    "    # Create interactive comparison of all three methods\n",
    "    fig = make_subplots(rows=1, cols=3, \n",
    "                       subplot_titles=['PCA', 't-SNE', 'UMAP'],\n",
    "                       specs=[[{'type': 'scatter'}, {'type': 'scatter'}, {'type': 'scatter'}]])\n",
    "    \n",
    "    # Color mapping\n",
    "    color_map = {'Normal': 'blue', 'Benign': 'green', \n",
    "                'Low_Grade_Cancer': 'orange', 'High_Grade_Cancer': 'red'}\n",
    "    \n",
    "    methods_plot = [\n",
    "        ('PCA', X_pca_2d, 1),\n",
    "        ('t-SNE', tsne_results[best_perplexity], 2),\n",
    "        ('UMAP', X_umap_best, 3)\n",
    "    ]\n",
    "    \n",
    "    for method_name, embedding, col_idx in methods_plot:\n",
    "        for tissue in tissue_types:\n",
    "            mask = y == tissue\n",
    "            fig.add_scatter(x=embedding[mask, 0], \n",
    "                          y=embedding[mask, 1],\n",
    "                          mode='markers',\n",
    "                          name=tissue,\n",
    "                          marker=dict(color=color_map[tissue], size=8, opacity=0.7),\n",
    "                          text=[f\"Patient: {pathology_data.iloc[i]['patient_id']}<br>Tissue: {tissue}\" \n",
    "                                for i in np.where(mask)[0]],\n",
    "                          hovertemplate='<b>%{text}</b><br>%{x:.2f}, %{y:.2f}<extra></extra>',\n",
    "                          showlegend=(col_idx == 1),  # Only show legend for first subplot\n",
    "                          row=1, col=col_idx)\n",
    "    \n",
    "    fig.update_layout(title_text=\"Interactive Dimensionality Reduction Comparison\", \n",
    "                     title_x=0.5,\n",
    "                     height=500,\n",
    "                     width=1200)\n",
    "    \n",
    "    # Update axes labels\n",
    "    fig.update_xaxes(title_text=\"Component 1\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Component 1\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\"Component 1\", row=1, col=3)\n",
    "    fig.update_yaxes(title_text=\"Component 2\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Component 2\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Component 2\", row=1, col=3)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Create 3D UMAP visualization\n",
    "    print(\"Creating 3D UMAP visualization...\")\n",
    "    \n",
    "    umap_3d = umap.UMAP(n_neighbors=int(best_config['n_neighbors']), \n",
    "                       min_dist=best_config['min_dist'],\n",
    "                       n_components=3, \n",
    "                       random_state=42)\n",
    "    \n",
    "    X_umap_3d = umap_3d.fit_transform(X_scaled)\n",
    "    \n",
    "    # Create 3D scatter plot\n",
    "    fig_3d = go.Figure()\n",
    "    \n",
    "    for tissue in tissue_types:\n",
    "        mask = y == tissue\n",
    "        fig_3d.add_scatter3d(x=X_umap_3d[mask, 0],\n",
    "                           y=X_umap_3d[mask, 1], \n",
    "                           z=X_umap_3d[mask, 2],\n",
    "                           mode='markers',\n",
    "                           name=tissue,\n",
    "                           marker=dict(color=color_map[tissue], size=5, opacity=0.8),\n",
    "                           text=[f\"Patient: {pathology_data.iloc[i]['patient_id']}<br>Tissue: {tissue}\" \n",
    "                                 for i in np.where(mask)[0]],\n",
    "                           hovertemplate='<b>%{text}</b><br>(%{x:.2f}, %{y:.2f}, %{z:.2f})<extra></extra>')\n",
    "    \n",
    "    fig_3d.update_layout(title='3D UMAP Visualization',\n",
    "                        scene=dict(xaxis_title='UMAP 1',\n",
    "                                  yaxis_title='UMAP 2', \n",
    "                                  zaxis_title='UMAP 3'),\n",
    "                        width=800,\n",
    "                        height=600)\n",
    "    \n",
    "    fig_3d.show()\n",
    "    \n",
    "    print(\"✅ Interactive visualizations created!\")\n",
    "    \n",
    "else:\n",
    "    print(\"Creating static 3D visualization...\")\n",
    "    \n",
    "    # Create 3D UMAP with matplotlib\n",
    "    umap_3d = umap.UMAP(n_neighbors=int(best_config['n_neighbors']), \n",
    "                       min_dist=best_config['min_dist'],\n",
    "                       n_components=3, \n",
    "                       random_state=42)\n",
    "    \n",
    "    X_umap_3d = umap_3d.fit_transform(X_scaled)\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 9))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    for tissue in tissue_types:\n",
    "        mask = y == tissue\n",
    "        ax.scatter(X_umap_3d[mask, 0], X_umap_3d[mask, 1], X_umap_3d[mask, 2],\n",
    "                  c=colors[tissue], label=tissue, s=30, alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('UMAP 1')\n",
    "    ax.set_ylabel('UMAP 2') \n",
    "    ax.set_zlabel('UMAP 3')\n",
    "    ax.set_title('3D UMAP Visualization')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✅ 3D visualization created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf865b14",
   "metadata": {},
   "source": [
    "## 8. Auto-Validation Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edec921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-validation tests for dimensionality reduction\n",
    "print(\"=== AUTO-VALIDATION TESTS ===\")\n",
    "\n",
    "# Test 1: Embedding dimensions\n",
    "assert X_pca_2d.shape == (n_samples, 2), \"❌ PCA embedding wrong dimensions\"\n",
    "assert X_tsne_best.shape == (n_samples, 2), \"❌ t-SNE embedding wrong dimensions\" \n",
    "assert X_umap_best.shape == (n_samples, 2), \"❌ UMAP embedding wrong dimensions\"\n",
    "print(\"✅ Test 1 passed: All embeddings have correct dimensions\")\n",
    "\n",
    "# Test 2: No NaN values in embeddings\n",
    "assert not np.isnan(X_pca_2d).any(), \"❌ NaN values in PCA embedding\"\n",
    "assert not np.isnan(X_tsne_best).any(), \"❌ NaN values in t-SNE embedding\"\n",
    "assert not np.isnan(X_umap_best).any(), \"❌ NaN values in UMAP embedding\"\n",
    "print(\"✅ Test 2 passed: No NaN values in embeddings\")\n",
    "\n",
    "# Test 3: PCA explained variance properties\n",
    "assert 0 < pca_2d.explained_variance_ratio_[0] < 1, \"❌ Invalid PC1 explained variance\"\n",
    "assert 0 < pca_2d.explained_variance_ratio_[1] < 1, \"❌ Invalid PC2 explained variance\"\n",
    "assert pca_2d.explained_variance_ratio_[0] >= pca_2d.explained_variance_ratio_[1], \"❌ PC1 should explain more variance than PC2\"\n",
    "print(\"✅ Test 3 passed: PCA explained variance is valid\")\n",
    "\n",
    "# Test 4: Clustering validation\n",
    "for method_name, cluster_labels in cluster_results.items():\n",
    "    n_unique_clusters = len(np.unique(cluster_labels[cluster_labels != -1]))\n",
    "    assert n_unique_clusters >= 1, f\"❌ {method_name} produced no valid clusters\"\n",
    "    assert n_unique_clusters <= n_samples, f\"❌ {method_name} produced too many clusters\"\n",
    "print(\"✅ Test 4 passed: All clustering methods produced valid results\")\n",
    "\n",
    "# Test 5: Silhouette scores are in valid range\n",
    "for method in ['PCA', 't-SNE', 'UMAP']:\n",
    "    method_embedding = methods_data[method]\n",
    "    sil_score = silhouette_score(method_embedding, y_numeric)\n",
    "    assert -1 <= sil_score <= 1, f\"❌ Invalid silhouette score for {method}\"\n",
    "print(\"✅ Test 5 passed: All silhouette scores in valid range\")\n",
    "\n",
    "# Test 6: Feature scaling verification\n",
    "assert np.allclose(X_scaled.mean(axis=0), 0, atol=1e-10), \"❌ Features not properly centered\"\n",
    "assert np.allclose(X_scaled.std(axis=0), 1, atol=1e-10), \"❌ Features not properly scaled\"\n",
    "print(\"✅ Test 6 passed: Data preprocessing is correct\")\n",
    "\n",
    "# Test 7: Hyperparameter optimization results\n",
    "assert best_config['silhouette_score'] > -1, \"❌ Best silhouette score too low\"\n",
    "assert 0 < best_config['n_neighbors'] <= 100, \"❌ Invalid best n_neighbors\"\n",
    "assert 0 <= best_config['min_dist'] <= 1, \"❌ Invalid best min_dist\"\n",
    "print(\"✅ Test 7 passed: Hyperparameter optimization successful\")\n",
    "\n",
    "# Test 8: Method comparison consistency\n",
    "methods_comparison = metrics_df\n",
    "assert len(methods_comparison) == 3, \"❌ Should compare exactly 3 methods\"\n",
    "assert all(0 <= score <= 1 for score in methods_comparison['Silhouette_Score']), \"❌ Invalid silhouette scores\"\n",
    "assert all(-1 <= score <= 1 for score in methods_comparison['Rank_Correlation']), \"❌ Invalid rank correlations\"\n",
    "print(\"✅ Test 8 passed: Method comparison metrics are valid\")\n",
    "\n",
    "print(f\"\\n🎉 All validation tests passed! You've successfully mastered dimensionality reduction techniques!\")\n",
    "print(f\"Summary of achievements:\")\n",
    "print(f\"• ✅ Applied PCA, t-SNE, and UMAP to {n_samples} pathology samples\")\n",
    "print(f\"• ✅ Optimized hyperparameters for each method\")\n",
    "print(f\"• ✅ Compared methods using multiple metrics\")\n",
    "print(f\"• ✅ Discovered {len(tissue_types)} tissue type clusters\")\n",
    "print(f\"• ✅ Best method: {best_overall} (silhouette score: {metrics_df.set_index('Method').loc[best_overall, 'Silhouette_Score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d7c51b",
   "metadata": {},
   "source": [
    "## 9. Next Steps and Advanced Applications\n",
    "\n",
    "Congratulations! You've mastered advanced dimensionality reduction techniques for digital pathology data.\n",
    "\n",
    "**Key Skills Acquired:**\n",
    "✅ PCA for linear dimensionality reduction  \n",
    "✅ t-SNE for local structure preservation  \n",
    "✅ UMAP for balanced local/global structure  \n",
    "✅ Hyperparameter optimization and validation  \n",
    "✅ Method comparison and selection  \n",
    "✅ Cluster discovery and analysis  \n",
    "✅ Interactive visualization techniques  \n",
    "✅ Quality metrics and validation  \n",
    "\n",
    "**In your next advanced notebooks, you'll learn:**\n",
    "- Foundation models (UNI, CONCH, CLAM) integration\n",
    "- Whole slide image analysis workflows\n",
    "- Computational pathology pipelines\n",
    "- Spatial transcriptomics analysis\n",
    "- Multi-modal data integration\n",
    "\n",
    "**For Further Practice:**\n",
    "- Apply to real TCGA datasets from different cancer types\n",
    "- Experiment with other distance metrics (cosine, manhattan)\n",
    "- Try ensemble dimensionality reduction approaches\n",
    "- Create interactive dashboards for pathology exploration\n",
    "- Integrate with deep learning feature extractors\n",
    "\n",
    "**Clinical Applications:**\n",
    "- Patient stratification and subtyping\n",
    "- Biomarker discovery workflows\n",
    "- Treatment response prediction\n",
    "- Pathology image quality assessment\n",
    "- Multi-omics data integration\n",
    "- Precision medicine applications\n",
    "\n",
    "**Advanced Techniques to Explore:**\n",
    "- Parametric UMAP for new sample projection\n",
    "- Supervised dimensionality reduction\n",
    "- Time-series embedding for longitudinal studies\n",
    "- Graph-based dimensionality reduction\n",
    "- Deep autoencoders for non-linear embedding"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
